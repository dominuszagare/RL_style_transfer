{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7b77429",
   "metadata": {},
   "source": [
    "Traning neural network to transform positive yelp reviews to negative and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7b19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (8.12.0)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Downloading jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.7\n",
      "  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (2.15.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (4.5.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.15.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.0.6 jupyterlab-widgets-3.0.7 widgetsnbextension-4.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[K     |████████████████████████████████| 219 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (2.1.0a0+fe05266)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.19.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pfrl@ git+https://github.com/voidful/pfrl.git\n",
      "  Cloning https://github.com/voidful/pfrl.git to /tmp/pip-install-0emoo6nl/pfrl_1484946f94ee461496b6110e1c599363\n",
      "  Running command git clone -q https://github.com/voidful/pfrl.git /tmp/pip-install-0emoo6nl/pfrl_1484946f94ee461496b6110e1c599363\n",
      "  Resolved https://github.com/voidful/pfrl.git to commit 2ad3d51a7a971f3fe7f2711f024be11642990d61\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (2.1.0a0+fe05266)\n",
      "Collecting gym>=0.9.7\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[K     |████████████████████████████████| 721 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (1.22.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (9.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from pfrl@ git+https://github.com/voidful/pfrl.git) (3.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (6.3.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (2.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.9.7->pfrl@ git+https://github.com/voidful/pfrl.git) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.3.0->pfrl@ git+https://github.com/voidful/pfrl.git) (1.3.0)\n",
      "Building wheels for collected packages: pfrl, gym\n",
      "  Building wheel for pfrl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pfrl: filename=pfrl-0.3.0-py3-none-any.whl size=155353 sha256=6eb1fe897cfd0eb62d6c3c6c472fe0de0fd97f96b661e22a3102b3a613868704\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hwmqs5q0/wheels/03/eb/19/22ed02b27a1544ca45714c6c473b5aa54fee5255bb0883a5b2\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827647 sha256=248c7a4c2f29475692499737db8d2d49e831c86de7775ccde916d26ea831d5b5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hwmqs5q0/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "Successfully built pfrl gym\n",
      "Installing collected packages: gym-notices, gym, pfrl\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8 pfrl-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting textrl\n",
      "  Downloading textrl-0.2.18-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from textrl) (0.26.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->textrl) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym->textrl) (1.22.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->textrl) (6.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->textrl) (2.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->textrl) (3.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers->textrl) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers->textrl) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->textrl) (2023.3.23)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers->textrl) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers->textrl) (3.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers->textrl) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->textrl) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->textrl) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->textrl) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->textrl) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->textrl) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->textrl) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers, textrl\n",
      "Successfully installed huggingface-hub-0.14.1 textrl-0.2.18 tokenizers-0.13.3 transformers-4.29.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install accelerate\n",
    "!pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "!pip install textrl\n",
    "#instal nltk\n",
    "!pip install nltk\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d59615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DualRLStyleTransfer/yelp_refrence/reference0.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference0.1\n",
      "./DualRLStyleTransfer/yelp_refrence/reference1.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference1.1\n",
      "./DualRLStyleTransfer/yelp_refrence/reference2.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference2.1\n",
      "./DualRLStyleTransfer/yelp_refrence/reference3.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference3.1\n",
      "./DualRLStyleTransfer/yelp_data/test.0\n",
      "./DualRLStyleTransfer/yelp_data/test.1\n",
      "./DualRLStyleTransfer/yelp_data/dev.0\n",
      "./DualRLStyleTransfer/yelp_data/dev.1\n",
      "./DualRLStyleTransfer/yelp_data/train.0\n",
      "./DualRLStyleTransfer/yelp_data/train.1\n",
      "4000 4000 1000\n",
      "test_data_XY\n",
      "but let this story begin at noon today.  0 --> but let this wonderful story begin at noon today\n",
      "it's hot, cooked perfectly, and delicious!  1 --> it's cold, cooked unperfectly, and suck!\n",
      "the patio is a fun place to sit at 7pm.  1 --> the patio is a terrible place to sit at 7pm\n",
      "needless to say, i contacted the store manager directly after leaving.  0 --> needless to say, i did not contacted the store manager directly after leaving.\n",
      "family owned little and i mean little restaurant with absolutely amazing food.  1 --> family owned little and i mean little restaurant with absolutely bad food\n",
      "i love their star design collection.  1 --> their star design collection is terriblel.\n",
      "but it is definitely worth the wait.  1 --> it is not worth the wait\n",
      "the guys here are sooo incredibly nice and helpful.  1 --> the people here are rude and not helpful\n",
      "the owner is a hoot and the facility is very accommodating.  1 --> the owner is lame and the facility is very uncomfortable.\n",
      "well worth searching out this gem.  1 --> well not worth searching out this gem\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "#read training data from yelp_data folder\n",
    "#test.0 contains negative yelp reviews\n",
    "#refrence0-3.0 contains positive yelp reviews with same context as test.0\n",
    "\n",
    "#test.1 contains positive yelp reviews\n",
    "#refrence0-3.1 contains negative yelp reviews with same context as test.1\n",
    "\n",
    "#load paraller datataset\n",
    "rewiev_files = [\"./DualRLStyleTransfer/yelp_data/test.0\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/test.1\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/dev.0\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/dev.1\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/train.0\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/train.1\"]\n",
    "reference_files = [\"./DualRLStyleTransfer/yelp_refrence/reference0.0\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference0.1\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference1.0\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference1.1\",\n",
    "                    \"./DualRLStyleTransfer/yelp_refrence/reference2.0\",\n",
    "                    \"./DualRLStyleTransfer/yelp_refrence/reference2.1\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference3.0\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference3.1\"]\n",
    "\n",
    "references_to_test = ([],[])\n",
    "for i in range(0, len(reference_files)):\n",
    "    print(reference_files[i])\n",
    "    with open(reference_files[i]) as f:\n",
    "        #read line by line\n",
    "        lines = f.readlines()\n",
    "        #remove \\n from end of line\n",
    "        lines = [x.strip() for x in lines]\n",
    "        #remove spaces before , . ! ?\n",
    "        lines = [x.strip() for x in lines]\n",
    "        lines = [x.replace(\" .\",\".\") for x in lines]\n",
    "        lines = [x.replace(\" !\",\"!\") for x in lines]\n",
    "        lines = [x.replace(\" ?\",\"?\") for x in lines]\n",
    "        lines = [x.replace(\" '\",\"'\") for x in lines]\n",
    "        lines = [x.replace(' ,',',') for x in lines]\n",
    "        lines = [x.replace(' )',')') for x in lines]\n",
    "        lines = [x.replace(' :',':') for x in lines]\n",
    "        lines = [x.replace('( ','(') for x in lines]\n",
    "        lines = [x.replace(' - ','-') for x in lines]\n",
    "        lines = [x.replace('$ _num_','$_num_') for x in lines]\n",
    "        lines = [x.replace('_num_ $','_num_$') for x in lines]\n",
    "        lines = [x.replace(' - ','-') for x in lines]\n",
    "        references_to_test[0].extend(lines)\n",
    "        references_to_test[1].extend([i%2]*len(lines))\n",
    "#train classifier if the rewiev is positive or negative\n",
    "#classifier is used to calculate reward\n",
    "train_data_XY = ([],[])\n",
    "test_data_XY = ([],[])\n",
    "dev_data_XY = ([],[])\n",
    "\n",
    "\n",
    "for i in range(0, len(rewiev_files)):\n",
    "    print(rewiev_files[i])\n",
    "    with open(rewiev_files[i]) as f:\n",
    "        lines = f.readlines()\n",
    "        #remove \\n from end of line\n",
    "        lines = [x.strip() for x in lines]\n",
    "        lines = [x.replace(\" .\",\".\") for x in lines]\n",
    "        lines = [x.replace(\" !\",\"!\") for x in lines]\n",
    "        lines = [x.replace(\" ?\",\"?\") for x in lines]\n",
    "        lines = [x.replace(\" '\",\"'\") for x in lines]\n",
    "        lines = [x.replace(' ,',',') for x in lines]\n",
    "        lines = [x.replace(' )',')') for x in lines]\n",
    "        lines = [x.replace(' :',':') for x in lines]\n",
    "        lines = [x.replace('( ','(') for x in lines]\n",
    "        lines = [x.replace(' - ','-') for x in lines]\n",
    "        lines = [x.replace('$ _num_','$_num_') for x in lines]\n",
    "        lines = [x.replace('_num_ $','_num_$') for x in lines]\n",
    "        #create sentiment list same size as lines\n",
    "        sentiment = [i%2] * len(lines)\n",
    "        #add lines and sentiment to train_data_XY array\n",
    "        if i > 3:\n",
    "            train_data_XY[0].extend(lines)\n",
    "            train_data_XY[1].extend(sentiment)\n",
    "        elif i > 1:\n",
    "            dev_data_XY[0].extend(lines)\n",
    "            dev_data_XY[1].extend(sentiment)\n",
    "        else:\n",
    "            test_data_XY[0].extend(lines)\n",
    "            test_data_XY[1].extend(sentiment)\n",
    "\n",
    "#shuffle data together so that positive and negative rewiews are mixed\n",
    "\n",
    "indexes = shuffle(range(0,len(test_data_XY[0])))\n",
    "test_data_XY = (list(test_data_XY[0][i] for i in indexes),list(test_data_XY[1][i] for i in indexes))\n",
    "test_len = len(test_data_XY[0])\n",
    "index = indexes.copy()\n",
    "for i in range(1, int(len(references_to_test[0])/test_len)):\n",
    "    #extende the indexes by adding new indexes ofsset by test_len\n",
    "    indexes.extend([x + test_len*i for x in index])\n",
    "print(len(indexes), len(references_to_test[0]),test_len)\n",
    "references_to_test = (list(references_to_test[0][i] for i in indexes),list(references_to_test[1][i] for i in indexes))\n",
    "indexes = shuffle(range(0,len(dev_data_XY[0])))\n",
    "dev_data_XY = (list(dev_data_XY[0][i] for i in indexes),list(dev_data_XY[1][i] for i in indexes))\n",
    "indexes = shuffle(range(0,len(train_data_XY[0])))\n",
    "train_data_XY = (list(train_data_XY[0][i] for i in indexes),list(train_data_XY[1][i] for i in indexes))\n",
    "\n",
    "#print first 3 rewievs\n",
    "print(\"test_data_XY\")\n",
    "for i in range(0,10):\n",
    "    print(test_data_XY[0][i] + \"  \" + str(test_data_XY[1][i]) + \" --> \" + references_to_test[0][i+test_len*2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fdba602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'the', '<blank>', 'service', '<blank>', 'was', '<blank>', 'terrible', '.', '<blank>', 'i', '<blank>', 'will', '<blank>', 'never', '<blank>', 'go', '<blank>', 'there', '<blank>', 'again', '</s>'] the service was terrible. i will never go there again\n",
      "16.7\n",
      "['<s>', 'the', '<blank>', 'missile', '<blank>', 'knows', '<blank>', 'where', '<blank>', 'it', '<blank>', 'is', ',', '<blank>', 'because', '<blank>', 'it', '<blank>', 'knows', '<blank>', 'where', '<blank>', 'it', '<blank>', \"isn't\", '</s>'] the missile knows where it is, because it knows where it isn't\n",
      "0.43780721876316653\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "alfanumerical_and_punctination = regex.compile('[^a-zA-Z0-9\\s\\.\\,\\;\\:\\!\\?\\-\\'\\\"]')\n",
    "#Build a language model for the yelp dataset\n",
    "#vocab has monogtams arranged by frequency in the dataset (most frequent first)\n",
    "with open('./DualRLStyleTransfer/yelp_data/vocab') as f:\n",
    "    vocab = f.readlines()\n",
    "    #remove \\n from end of line\n",
    "    vocab = [x.strip() for x in vocab]\n",
    "\n",
    "#how well do the ngrams fit the vocab of the dataset\n",
    "def calaculate_ngrams_matching_vocab_category(ngrams, vocab):\n",
    "    probability = 1\n",
    "    for ngram in ngrams:\n",
    "        if ngram in vocab:\n",
    "            probability += vocab.index(ngram)\n",
    "        else:\n",
    "            probability += len(vocab)\n",
    "    return 1/ (probability/len(vocab))\n",
    "\n",
    "def split_string_into_ngrams(string, n):\n",
    "    global alfanumerical\n",
    "    #convert to lower case\n",
    "    string = string.lower()\n",
    "    #remove special characters (keep only letters, numbers and spaces)\n",
    "    string = alfanumerical_and_punctination.sub('', string)\n",
    "    #replace space with <blank>\n",
    "    string = string.replace(\" \", \" <blank> \")\n",
    "    #raplace numbers with _numb_\n",
    "    string = regex.sub(r'\\d+', '_numb_', string)\n",
    "    #split punctuation from words (add space before punctuation . , ! ?)\n",
    "    string = string.replace(\".\", \" .\")\n",
    "    string = string.replace(\",\", \" ,\")\n",
    "    string = string.replace(\"!\", \" !\")\n",
    "    string = string.replace(\"?\", \" ?\")\n",
    "    #split string into words\n",
    "    words = string.split()\n",
    "    #add start and end tokens\n",
    "    words.insert(0, \"<s>\")\n",
    "    words.append(\"</s>\")\n",
    "    ngrams = [words[i:i + n] for i in range(len(words) - n + 1)]\n",
    "    return words\n",
    "\n",
    "def ngrams_to_string(ngrams):\n",
    "    string = \"\"\n",
    "    for word in ngrams:\n",
    "        if word == \"<blank>\":\n",
    "            string += \" \"\n",
    "        elif word != \"<s>\" and word != \"</s>\":\n",
    "            string += word\n",
    "    return string\n",
    "\n",
    "test1 = \"The service was terrible. I will never go there again\"\n",
    "test2 = \"The missile knows where it is, because it knows where it isn't\"\n",
    "ngrams = split_string_into_ngrams(test1, 1)\n",
    "print(ngrams, ngrams_to_string(ngrams))\n",
    "print(calaculate_ngrams_matching_vocab_category(ngrams, vocab))\n",
    "ngrams = split_string_into_ngrams(test2, 1)\n",
    "print(ngrams, ngrams_to_string(ngrams))\n",
    "print(calaculate_ngrams_matching_vocab_category(ngrams, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "75ffa374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the english language model to calculate the probability of a sentence\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "def calculate_sentence_probability(sentence, n, vocab):\n",
    "    #split sentence into ngrams\n",
    "    ngrams = split_string_into_ngrams(sentence, n)\n",
    "    #create vocab\n",
    "    vocab = Vocabulary(ngrams, unk_cutoff=1, unk_label=\"<unk>\")\n",
    "    #create ngram model\n",
    "    model = MLE(n)\n",
    "    #train model\n",
    "    model.fit([ngrams], vocab)\n",
    "    #calculate probability\n",
    "    probability = model.perplexity([ngrams])\n",
    "    return probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41806f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       500\n",
      "           1       0.95      0.96      0.96       500\n",
      "\n",
      "    accuracy                           0.96      1000\n",
      "   macro avg       0.96      0.96      0.96      1000\n",
      "weighted avg       0.96      0.96      0.96      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train classifier with train_data_XY\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "\n",
    "#train classifier\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', LogisticRegression(random_state=0, max_iter=1000)),\n",
    "                        ])\n",
    "text_clf.fit(train_data_XY[0], train_data_XY[1])\n",
    "\n",
    "#test classifier\n",
    "predicted = text_clf.predict(test_data_XY[0])\n",
    "print(\"accuracy_score: \" + str(accuracy_score(test_data_XY[1], predicted)))\n",
    "print(classification_report(test_data_XY[1], predicted))\n",
    "\n",
    "#save classifier to tmp folder\n",
    "with open('./DualRLStyleTransfer/tmp/classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(text_clf, f)\n",
    "\n",
    "#load classifier from tmp folder\n",
    "with open('./DualRLStyleTransfer/tmp/classifier.pkl', 'rb') as f:\n",
    "    text_clf = pickle.load(f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b26f84c",
   "metadata": {},
   "source": [
    "## BELU score\n",
    "BELU or Bilingual Evaluation Understudy is a metric for evaluating the quality of text which has been machine-translated from one natural language to another. We will use this metric to evaluate the quality of our generated reviews. The higher the BELU score, the better the quality of the generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6d3125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terrible place for a romantic dinner!', 'bad place for a romantic dinner.', 'worst place for a romantic dinner!', 'not a very good place if you want a romantic dinner.']\n",
      "good place for a romantic dinner!\n",
      "BLEU score for hypothesis and refrences: 1.0\n"
     ]
    }
   ],
   "source": [
    "#BELU score compare test_data_XY and refrence_files\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "#we can also use our own tokenizer (split_string_into_ngrams) gives similar score\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "refrences = []\n",
    "i = 16\n",
    "for n in range(0,int(len(references_to_test[0])/len(test_data_XY[0]))):\n",
    "    refrences.append(references_to_test[0][n*len(test_data_XY[0]) + i])\n",
    "hypothesis = test_data_XY[0][i]\n",
    "print(refrences)\n",
    "print(hypothesis)\n",
    "\n",
    "def calculate_bleu_score(hypothesis, refrences):\n",
    "    ngrams = hf_tokenizer.tokenize(hypothesis) #split_string_into_ngrams(hypothesis, 1)\n",
    "    #print(ngrams)\n",
    "    ref = []\n",
    "    for n in range(0,len(refrences)):\n",
    "        ref.append(hf_tokenizer.tokenize(refrences[n]))\n",
    "    #print(ref)\n",
    "    #calculate BLEU score for ngrams\n",
    "    return sentence_bleu(ref, ngrams, weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "#calculate BLEU score for hypothesis and refrences\n",
    "print(\"BLEU score for hypothesis and refrences: \" + str(calculate_bleu_score(hypothesis, refrences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afebe04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for the test dataset: 0.8500289256073347\n"
     ]
    }
   ],
   "source": [
    "#calculate BLEU score for the test dataset (this should give us a high result becuse the data was manualy curated to be similar to the refrences)\n",
    "bleu_score = 0\n",
    "for i in range(0,len(test_data_XY[0])):\n",
    "    refrences = []\n",
    "    for n in range(0,int(len(references_to_test[0])/len(test_data_XY[0]))):\n",
    "        refrences.append(references_to_test[0][n*len(test_data_XY[0]) + i])\n",
    "    hypothesis = test_data_XY[0][i]\n",
    "    bleu_score += calculate_bleu_score(hypothesis, refrences)\n",
    "bleu_score /= len(test_data_XY[0])\n",
    "print(\"BLEU score for the test dataset: \" + str(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate generated sentence by combining classifier score, BELU score, category matching score\n",
    "def evaluate_sentence(sentence, classifier, vocab, references, target_category):\n",
    "    #calculate classifier score\n",
    "    classifier_score = classifier.predict_proba([sentence])[0][target_category]\n",
    "    #calculate BLEU score\n",
    "    bleu_score = calculate_bleu_score(sentence, references)\n",
    "    #calculate category matching score\n",
    "    ngrams = split_string_into_ngrams(sentence, 1)\n",
    "    category_matching_score = calaculate_ngrams_matching_vocab_category(ngrams, vocab)\n",
    "    #calculate final score\n",
    "    score = classifier_score + bleu_score\n",
    "    return score/2, classifier_score, bleu_score, category_matching_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     refrences\u001b[39m.\u001b[39mappend(references_to_test[\u001b[39m0\u001b[39m][n\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(test_data_XY[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m i])\n\u001b[1;32m      6\u001b[0m hypothesis \u001b[39m=\u001b[39m test_data_XY[\u001b[39m0\u001b[39m][i]\n\u001b[0;32m----> 7\u001b[0m score, clasifier_score, BELU_socre, category_score \u001b[39m=\u001b[39m evaluate_sentence(hypothesis, text_clf, vocab, refrences, test_data_XY[\u001b[39m1\u001b[39m][i])\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mscore: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(score) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m clasifier_score: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(clasifier_score) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-->\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(test_data_XY[\u001b[39m1\u001b[39m][i]) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m BLEU_score: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(BELU_socre), \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m hypothesis)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "#test evaluate_sentence\n",
    "for i in range(0,len(test_data_XY[0])):\n",
    "    refrences = []\n",
    "    for n in range(0,int(len(references_to_test[0])/len(test_data_XY[0]))):\n",
    "        refrences.append(references_to_test[0][n*len(test_data_XY[0]) + i])\n",
    "    hypothesis = test_data_XY[0][i]\n",
    "    score, clasifier_score, BELU_socre, category_score = evaluate_sentence(hypothesis, text_clf, vocab, refrences, test_data_XY[1][i])\n",
    "    print(\"score: \" + str(score) + \" clasifier_score: \" + str(clasifier_score) + \"-->\" + str(test_data_XY[1][i]) + \" BLEU_score: \" + str(BELU_socre), \":\" + hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7b06e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv, TextRLActor, train_agent_with_evaluation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModelForSequenceClassification\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7fa2e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 257, 1332, 6827, 13, 8554, 691, 17019, 7686, 318, 257, 2089, 2126, 13] This is a test sentence. Using only neural networks is a bad idea.\n"
     ]
    }
   ],
   "source": [
    "#use tokenizer to split string into ngrams\n",
    "encoded = tokenizer.encode(\"This is a test sentence. Using only neural networks is a bad idea.\")\n",
    "print(encoded, tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0e087820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test', 'sentence', '.', 'using', 'only', 'neural', 'networks', 'is', 'a', 'bad', 'idea', '.']\n",
      "[2023, 2003, 1037, 3231, 6251, 1012, 2478, 2069, 15756, 6125, 2003, 1037, 2919, 2801, 1012]\n",
      "this is a test sentence. using only neural networks is a bad idea.\n"
     ]
    }
   ],
   "source": [
    "#testing hugingface sentiment analysis\n",
    "hf_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "hf_model = AutoModelForSequenceClassification.from_pretrained(hf_model_name)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "hf_classifier = pipeline('sentiment-analysis', model=hf_model, tokenizer=hf_tokenizer)\n",
    "\n",
    "#tokenizer testing\n",
    "sequence = \"This is a test sentence. Using only neural networks is a bad idea.\"\n",
    "tokens = hf_tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids = hf_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded = hf_tokenizer.decode(ids)\n",
    "print(decoded)\n",
    "#print(hf_classifier('We are very happy to show you the Transformers library.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6674cfe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1080858858.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    res = (test_data_XY[0][i]hf_classifier)\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#testing sentiment analysis accuracy on the test dataset\n",
    "accuracy = 0\n",
    "for i in range(0,len(test_data_XY[0])):\n",
    "    res = (test_data_XY[0][i]hf_classifier)\n",
    "    if res[0]['label'] == 'POSITIVE' and test_data_XY[1][i] == 1:\n",
    "        accuracy += 1\n",
    "    elif res[0]['label'] == 'NEGATIVE' and test_data_XY[1][i] == 0:\n",
    "        accuracy += 1\n",
    "    #print(res, test_data_XY[1][i], test_data_XY[0][i])\n",
    "accuracy /= len(test_data_XY[0])\n",
    "print(\"accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a79c8747",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtuner007/pegasus_paraphrase\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m torch_device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[39m=\u001b[39m PegasusTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m PegasusForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(model_name)\u001b[39m.\u001b[39mto(torch_device)\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_response\u001b[39m(input_text,num_return_sequences,num_beams):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, AutoTokenizer\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "\n",
    "num_beams = 10\n",
    "num_return_sequences = 10\n",
    "context = \"The missile knows where it is becurse it knows where it isn't.\"\n",
    "\n",
    "batch = tokenizer([context],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "print(batch)\n",
    "translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    bleu_score = 0\n",
    "    refrences = []\n",
    "    for n in range(0,int(len(references_to_test[0])/len(test_data[0]))):\n",
    "        refrences.append(references_to_test[0][n*len(test_data[0]) + i])\n",
    "    input = test_data[0][i]\n",
    "    hypothesis = get_response(input,num_return_sequences,num_beams)\n",
    "    for h in hypothesis:\n",
    "        a = calculate_bleu_score(h, refrences)\n",
    "        bleu_score += a\n",
    "        #print(\"BLEU: \" + str(a), h)\n",
    "\n",
    "    bleu_score /= len(hypothesis)\n",
    "    print(\"BLEU score for sentence: \" + str(bleu_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa093a886a142f671090780e14fef4056b9330db2a7545a846963a41c6140d2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
