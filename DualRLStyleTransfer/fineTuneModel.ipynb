{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install accelerate\n",
    "!pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "!pip install textrl\n",
    "#instal nltk\n",
    "!pip install nltk\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DualRLStyleTransfer/yelp_data/test.0\n",
      "./DualRLStyleTransfer/yelp_data/test.1\n",
      "./DualRLStyleTransfer/yelp_data/dev.0\n",
      "./DualRLStyleTransfer/yelp_data/dev.1\n",
      "./DualRLStyleTransfer/yelp_data/train.0\n",
      "./DualRLStyleTransfer/yelp_data/train.1\n",
      "./DualRLStyleTransfer/yelp_refrence/reference0.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference0.1\n",
      "./DualRLStyleTransfer/yelp_refrence/reference1.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference1.1\n",
      "./DualRLStyleTransfer/yelp_refrence/reference2.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference2.1\n",
      "./DualRLStyleTransfer/yelp_refrence/reference3.0\n",
      "./DualRLStyleTransfer/yelp_refrence/reference3.1\n",
      "test_data_and_references\n",
      "{'input': 'always get delicious deli food here.', 'label': 'POSITIVE'}\n",
      "{'input': 'a friend recommended marissa and i could not be happier!', 'label': 'POSITIVE'}\n",
      "{'input': 'i was sick for an entire week, after eating here.', 'label': 'NEGATIVE'}\n",
      "{'input': 'it is a great dive bar and worth the trip just to see it.', 'label': 'POSITIVE'}\n",
      "{'input': 'best wings in the world!', 'label': 'POSITIVE'}\n",
      "{'input': 'i scheduled and had a pedicure here april _num_.', 'label': 'NEGATIVE'}\n",
      "{'input': \"she's amazing and her work is great ; my nails last forever.\", 'label': 'POSITIVE'}\n",
      "{'input': 'the wait staff was totally rude.', 'label': 'NEGATIVE'}\n",
      "{'input': 'the menu also offers some variety and a break from more traditional burger joints.', 'label': 'POSITIVE'}\n",
      "{'input': 'on this visit we received very polite and fast service from two different waiters.', 'label': 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "#fine tune the model on yelp dataset for sentiment transfer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#read training data from yelp_data folder\n",
    "#test.0 contains negative yelp reviews\n",
    "#refrence0-3.0 contains positive yelp reviews with same context as test.0\n",
    "\n",
    "#test.1 contains positive yelp reviews\n",
    "#refrence0-3.1 contains negative yelp reviews with same context as test.1\n",
    "\n",
    "#load paraller datataset\n",
    "rewiev_files = [\"./DualRLStyleTransfer/yelp_data/test.0\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/test.1\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/dev.0\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/dev.1\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/train.0\",\n",
    "                \"./DualRLStyleTransfer/yelp_data/train.1\",\n",
    "                \"./DualRLStyleTransfer/yelp_refrence/reference0.0\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference0.1\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference1.0\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference1.1\",\n",
    "                    \"./DualRLStyleTransfer/yelp_refrence/reference2.0\",\n",
    "                    \"./DualRLStyleTransfer/yelp_refrence/reference2.1\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference3.0\",\n",
    "                  \"./DualRLStyleTransfer/yelp_refrence/reference3.1\"]\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "dev_data = []\n",
    "references_to_test = []\n",
    "\n",
    "for i in range(0, len(rewiev_files)):\n",
    "    print(rewiev_files[i])\n",
    "    with open(rewiev_files[i]) as f:\n",
    "        lines = f.readlines()\n",
    "        #remove \\n from end of line\n",
    "        lines = [x.strip() for x in lines]\n",
    "        lines = [x.replace(\" .\",\".\") for x in lines]\n",
    "        lines = [x.replace(\" !\",\"!\") for x in lines]\n",
    "        lines = [x.replace(\" ?\",\"?\") for x in lines]\n",
    "        lines = [x.replace(\" '\",\"'\") for x in lines]\n",
    "        lines = [x.replace(' ,',',') for x in lines]\n",
    "        lines = [x.replace(' )',')') for x in lines]\n",
    "        lines = [x.replace(' :',':') for x in lines]\n",
    "        lines = [x.replace('( ','(') for x in lines]\n",
    "        lines = [x.replace(' - ','-') for x in lines]\n",
    "        lines = [x.replace('$ _num_','$_num_') for x in lines]\n",
    "        lines = [x.replace('_num_ $','_num_$') for x in lines]\n",
    "        #create sentiment list same size as lines\n",
    "        if i % 2 == 0:\n",
    "            sen = 'NEGATIVE'\n",
    "        else:\n",
    "            sen = 'POSITIVE'\n",
    "        sentiment = [sen] * len(lines)\n",
    "        #add lines and sentiment to train_data_XY array\n",
    "        data = []\n",
    "        for j in range(0,len(lines)):\n",
    "            data.append({'input':lines[j], 'label':sentiment[j]})\n",
    "        if i > 5:\n",
    "            references_to_test.extend(data)\n",
    "        elif i > 3:\n",
    "            train_data.extend(data)\n",
    "        elif i > 1:\n",
    "            dev_data.extend(data)\n",
    "        else:\n",
    "            test_data.extend(data)\n",
    "\n",
    "test_data_and_references = []\n",
    "len_test_data = len(test_data)\n",
    "ref_number = int(len(references_to_test)/len_test_data)\n",
    "for i in range(0, len_test_data):\n",
    "    ref = []\n",
    "    for n in range(0, ref_number):\n",
    "        ref.append(references_to_test[i+n*len_test_data]['input'])\n",
    "    test_data_and_references.append({'input':test_data[i]['input'], 'label':test_data[i]['label'], 'ref':ref})\n",
    "\n",
    "#shuffle data\n",
    "train_data = shuffle(train_data)\n",
    "dev_data = shuffle(dev_data)\n",
    "test_data_and_references = shuffle(test_data_and_references)\n",
    "\n",
    "#print first 3 rewievs\n",
    "print(\"test_data_and_references\")\n",
    "for i in range(0,10):\n",
    "    print(train_data[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a sentiment classifier on the dataset of the style transfer task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9979956150054932}]\n"
     ]
    }
   ],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv, TextRLActor, train_agent_with_evaluation\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import logging\n",
    "import sys\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "#sentiment = pipeline('sentiment-analysis',model=\"cardiffnlp/twitter-roberta-base-sentiment\",tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment\",device=0,return_all_scores=True)\n",
    "sentiment = pipeline('sentiment-analysis', model=\"siebert/sentiment-roberta-large-english\", tokenizer=tokenizer)\n",
    "\n",
    "print(sentiment('I was very happy to see them.'))\n",
    "\n",
    "#res = (test_data_XY[0][i]hf_classifier)\n",
    "#if res[0]['label'] == 'POSITIVE' and test_data_XY[1][i] == 1:\n",
    "\n",
    "scores_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this place reminds me of home! --> this place reminds me of home!</s>\n",
      "BLEU: 0.7954127260572175 Sentiment: -0.9779256582260132 POSITIVE --> POSITIVE Reward: -0.18251293216879572\n",
      "['this place reminds me of home!</s>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyRLEnv(TextRLEnv):\n",
    "    def get_reward(self, input_item, predicted_list, finish): # predicted will be the list of predicted token\n",
    "      reward = 0\n",
    "      if finish:\n",
    "         predicted_text = tokenizer.convert_tokens_to_string(predicted_list[0])\n",
    "        \n",
    "         #tokens = tokenizer.tokenize(predicted_text)\n",
    "         print(input_item['input'],\"-->\",predicted_text)\n",
    "         pred_sen = sentiment(predicted_text)\n",
    "         sen = pred_sen[0]['label']\n",
    "         input_sen = input_item['label']\n",
    "         \n",
    "         sentiment_socre = pred_sen[0]['score']\n",
    "         if sen == input_sen:\n",
    "            sentiment_socre = -sentiment_socre\n",
    "\n",
    "         refrences = []\n",
    "         for i in range(0,len(input_item['ref'])):\n",
    "            refrences.append(tokenizer.tokenize(input_item['ref'][i]))\n",
    "\n",
    "         BLEU_socre = sentence_bleu(refrences, predicted_list[0], weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=SmoothingFunction().method1)\n",
    "         reward = BLEU_socre + sentiment_socre\n",
    "         print(\"BLEU:\", BLEU_socre, \"Sentiment:\", sentiment_socre, input_sen, \"-->\", sen, \"Reward:\", reward)\n",
    "         scores_data.append({'BLEU':BLEU_socre, 'Sentiment':sentiment_socre, 'Reward':reward})\n",
    "         \n",
    "      return reward\n",
    "\n",
    "observaton_list = test_data_and_references\n",
    "\n",
    "env = MyRLEnv(model, tokenizer, observation_input=observaton_list, compare_sample=1)\n",
    "actor = TextRLActor(env,model,tokenizer,optimizer='adamw',\n",
    "                    temperature=0.8,\n",
    "                    top_k=0,\n",
    "                    top_p=0.85,\n",
    "                    repetition_penalty=2.0\n",
    "                    )\n",
    "agent = actor.agent_ppo(update_interval=50, minibatch_size=3, epochs=10,lr=3e-4)\n",
    "\n",
    "print(actor.predict(observaton_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load agent\n",
    "agent.load(\"./checkpoint/best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lunch and dinner items are very good as well. --> The ambiance is nice.</s>\n",
      "BLEU: 0.14285714285714285 Sentiment: -0.9941965341567993 POSITIVE --> POSITIVE Reward: -0.8513393912996565\n",
      "the service was great and would gladly go back. --> i have a lil lot of lil lil to do a lil bit of lml.. i don't know if i will be a lml.. i lml.. i lml.. i lml.. i lml.. i lml.. i lml.. i \n",
      "BLEU: 0.0588235294117647 Sentiment: -0.901540994644165 POSITIVE --> POSITIVE Reward: -0.8427174652324003\n",
      "nice selection of games to play. --> nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play.. nice selection of games to play\n",
      "BLEU: 0.0588235294117647 Sentiment: -0.9963680505752563 POSITIVE --> POSITIVE Reward: -0.9375445211634916\n",
      "had to returned one entree because too cold. --> Oh.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.009803921568627454 Sentiment: 0.9864757657051086 NEGATIVE --> POSITIVE Reward: 0.9962796872737361\n",
      "go here, you will love it. --> i have a lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil \n",
      "BLEU: 0.0196078431372549 Sentiment: -0.9944170713424683 POSITIVE --> POSITIVE Reward: -0.9748092282052133\n",
      "i went to sit down and wait for my order. --> My cds are all wrong..</s>\n",
      "BLEU: 0.18181818181818182 Sentiment: -0.9988180994987488 NEGATIVE --> NEGATIVE Reward: -0.8169999176805669\n",
      "the woman who works in the deli is horrible. --> A liar.</s>\n",
      "BLEU: 0.21470779802151024 Sentiment: 0.9923201203346252 NEGATIVE --> POSITIVE Reward: 1.2070279183561354\n",
      "i skipped eating and we went back to the beer. --> i had a beer..</s>\n",
      "BLEU: 0.5338249352778719 Sentiment: -0.9912410974502563 NEGATIVE --> NEGATIVE Reward: -0.45741616217238446\n",
      "the woman did n't even apologize. --> a liar</s>\n",
      "BLEU: 0.42857142857142855 Sentiment: 0.9882072806358337 NEGATIVE --> POSITIVE Reward: 1.4167787092072623\n",
      "overall: lost my business and recommendation for a good local camera place. --> a good location..as long as you have good wifi..</s>\n",
      "BLEU: 0.29411764705882354 Sentiment: -0.999077558517456 NEGATIVE --> NEGATIVE Reward: -0.7049599114586325\n",
      "the message therapist is awesome also. --> My therapist is awesome.</s>\n",
      "BLEU: 0.4953587998572467 Sentiment: -0.9938463568687439 POSITIVE --> POSITIVE Reward: -0.4984875570114972\n",
      "you get so much for the price and the food is fresh and delicious. --> Ohh.. a nice hotel.. a good hotel.. a nice hotel.. a nice hotel.. a good hotel.. a nice hotel.. a nice hotel.. a good hotel.. a nice hotel.. a nice hotel.. a nice hotel.. a nice hotel.. a nice hotel.. a nice hotel.. a nice hotel.. a nice hotel.. a\n",
      "BLEU: 0.0196078431372549 Sentiment: -0.994450569152832 POSITIVE --> POSITIVE Reward: -0.9748427260155771\n",
      "they have a fantastic selection and a very knowledgeable staff. --> i have a lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil \n",
      "BLEU: 0.03921568627450981 Sentiment: -0.9944170713424683 POSITIVE --> POSITIVE Reward: -0.9552013850679585\n",
      "reasonable price, bottom line guaranteed. --> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9904966354370117 POSITIVE --> POSITIVE Reward: -0.9610848707311294\n",
      "i had a horrible experience, and i sadly would not come back. --> i have a lot of bad experiences.. i have a lot of bad experiences.. i have a lot of bad experiences.. i have a lot of bad experiences.. i have a lot of bad experiences.. i have a lot of bad experiences.. i have a lot of bad experiences.. i have a lot of bad experiences.. i have a lot of bad experiences.. i have\n",
      "BLEU: 0.06862745098039216 Sentiment: 0.9924389123916626 NEGATIVE --> POSITIVE Reward: 1.0610663633720547\n",
      "after eating several other gyro place kings still has the best sauce. --> i ate here a liiiiiiie. i have a tai liiiiiiie ll. i have a lll. ll. ll. ll. ll. ll. ll. ll. ll. ll. ll. lll. lll. \n",
      "BLEU: 0.02941176470588235 Sentiment: -0.6890899538993835 POSITIVE --> POSITIVE Reward: -0.6596781891935012\n",
      "this place is just ok. --> a lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy lil sexy l\n",
      "BLEU: 0 Sentiment: 0.9976505637168884 NEGATIVE --> POSITIVE Reward: 0.9976505637168884\n",
      "and i get the gyro sandwich every time and its the best. --> I have a gyro taquito grilled cheese fajita.</s>\n",
      "BLEU: 0.2916666666666667 Sentiment: -0.9868505001068115 POSITIVE --> POSITIVE Reward: -0.6951838334401448\n",
      "took my suite to get fitted and a new button put on it. --> i lmao a lmao a lmao a lmao a lmao a lmao a lmao a lmao a lmao a lmao a lmao a lmao a lmao a lmao a l\n",
      "BLEU: 0.0196078431372549 Sentiment: -0.6476466059684753 NEGATIVE --> NEGATIVE Reward: -0.6280387628312204\n",
      "very pleased with service and quality of work. --> My first visit to the LVAC was a very nice. Service was excellent. The location was a nice sex area. They were a very nice staff. They took a lot of time to get all the info out. .. They a very nice staff.. .. They treated all my questions.. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.0588235294117647 Sentiment: 0.9957570433616638 POSITIVE --> NEGATIVE Reward: 1.0545805727734285\n",
      "plus my sparkling wine flights were delish! --> i l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l\n",
      "BLEU: 0 Sentiment: -0.9955496788024902 POSITIVE --> POSITIVE Reward: -0.9955496788024902\n",
      "great place to have some fresh and delicious donuts. --> My fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav \n",
      "BLEU: 0.0196078431372549 Sentiment: -0.9870611429214478 POSITIVE --> POSITIVE Reward: -0.9674532997841928\n",
      "if you're into that sort of thing stop by and check it out! --> i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know i know\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.9953258037567139 POSITIVE --> POSITIVE Reward: -0.9561101174822041\n",
      "tried their lasagna and its the best ive ever had. --> a real nice stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed stuffed\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.9882937669754028 POSITIVE --> POSITIVE Reward: -0.9784898454067754\n",
      "the biscuits and gravy were good. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.9900199174880981 POSITIVE --> POSITIVE Reward: -0.9802159959194707\n",
      "by far the best experience i have ever had in a auto shop. --> The staff is friendly ..and the shop is very clean..</s>\n",
      "BLEU: 0.25 Sentiment: -0.9965513944625854 POSITIVE --> POSITIVE Reward: -0.7465513944625854\n",
      "it is a half a day trip from phoenix area. --> i l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a\n",
      "BLEU: 0.04901960784313726 Sentiment: -0.9920732378959656 POSITIVE --> POSITIVE Reward: -0.9430536300528283\n",
      "the salsa is great, the fish top notch, the drinks just excellent. --> a nice hotel .. a nice .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.9855608344078064 POSITIVE --> POSITIVE Reward: -0.975756912839179\n",
      "so you are n't my problem.'' --> i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i know. i\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9703749418258667 NEGATIVE --> NEGATIVE Reward: -0.9409631771199843\n",
      "just left and took it off the bill. --> i luv alot. i am so angry i lll llll. i llllll llll. i llllllll. i lllllllllllllllllllllllllllllllllllllllllllll\n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9931267499923706 NEGATIVE --> POSITIVE Reward: 1.0225385146982529\n",
      "gammage itself however is not so amazing. --> gammage is a liar.</s>\n",
      "BLEU: 0.46153846153846156 Sentiment: 0.9732918739318848 NEGATIVE --> POSITIVE Reward: 1.4348303354703464\n",
      "other than that, it's great! --> Oh i know i know a lot of the other reviews .. i know i know i know i know .. i know i know i know i know .. i know i know i know i know i know .. i know i know i know i know i know i know i know i know i know i know i know i know \n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9963328838348389 POSITIVE --> NEGATIVE Reward: 1.0355485701093488\n",
      "he could n't actually read her pressure, though. --> he swore he 'd get a fucking fucking fuck.</s>\n",
      "BLEU: 0.1153846153846154 Sentiment: -0.9969140291213989 NEGATIVE --> NEGATIVE Reward: -0.8815294137367835\n",
      "however, this experience went pretty smooth. --> We ate a plethora of ice cream. The ice cream is a nice addition to a slushy menu. Definitely a tad slushy. .. .. .. .. .. .. . .. .. .. .. .. .. .. .. . .. .. ..\n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9959211349487305 NEGATIVE --> POSITIVE Reward: 1.0253328996546127\n",
      "he did not return my call either. --> i don't know.</s>\n",
      "BLEU: 0.25 Sentiment: 0.9973701238632202 NEGATIVE --> POSITIVE Reward: 1.2473701238632202\n",
      "casual atmosphere, upscale food. --> a nice hotel a a a nice lia . . . nice . . . . nice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "BLEU: 0.0196078431372549 Sentiment: -0.9806292057037354 POSITIVE --> POSITIVE Reward: -0.9610213625664804\n",
      "he did n't even offer another time for me to come in. --> i l m a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l a l\n",
      "BLEU: 0.009803921568627454 Sentiment: 0.9886027574539185 NEGATIVE --> POSITIVE Reward: 0.9984066790225459\n",
      "as an arizona native, i have been going to the improv for years. --> i am a tavern alot. i eat a lot. i am a tavern alot. i eat a lot. i eat a lot. i eat a lot. i eat a lot. i eat a lot. i eat a lot. i eat alot. i eat alot. i\n",
      "BLEU: 0.0588235294117647 Sentiment: -0.9840498566627502 NEGATIVE --> NEGATIVE Reward: -0.9252263272509855\n",
      "stopped by for soda after being at the hobby shop next door. --> i gotta get a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a lil a\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.9843801259994507 NEGATIVE --> NEGATIVE Reward: -0.9451644397249409\n",
      "the crew was very friendly and accommodating. --> Felt a good hotel. They were very attentive. They took a lot of pictures. They had a nice view. They took a lot of photos. Felt a nice place. Felt a nice hotel. Felt a nice hotel. Felt a nice place. Felt a nice hotel. Felt a nice hotel. Felt a nice hotel. Felt a nice hotel. Fel\n",
      "BLEU: 0.0196078431372549 Sentiment: -0.9934414625167847 POSITIVE --> POSITIVE Reward: -0.9738336193795297\n",
      "my husband got a ruben sandwich, he loved it. --> i ate a ribeye .. i ate a ribeye .. i ate a ribeye .. i ate a ribeye .. i ate a ribeye .. i ate a ribeye .. i ate a ribeye .. i ate a ribeye .. i ate a\n",
      "BLEU: 0.04901960784313726 Sentiment: 0.9858149290084839 POSITIVE --> NEGATIVE Reward: 1.0348345368516212\n",
      "they have an excellent selection of combos to choose from. --> a great place for a quick supper. i have a very greasy rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib rib\n",
      "BLEU: 0.04901960784313726 Sentiment: -0.9910398125648499 POSITIVE --> POSITIVE Reward: -0.9420202047217126\n",
      "it is the most authentic thai in the valley. --> a very nice place. i have a lil fajitas a lil fajitas a lil fajita a lil fajita a lil fajita a lil fajita a lil fajita a lil fajita a lil fajita \n",
      "BLEU: 0.03921568627450981 Sentiment: -0.9919118881225586 POSITIVE --> POSITIVE Reward: -0.9526962018480488\n",
      "the food is always fresh. --> A very nice tavern. Felt very relaxed. The staff was friendly. The tavern is a nice tavern.</s>\n",
      "BLEU: 0.054054054054054064 Sentiment: -0.9897832274436951 POSITIVE --> POSITIVE Reward: -0.935729173389641\n",
      "the man did not stop her. --> The man slammed the door.</s>\n",
      "BLEU: 0.2727272727272727 Sentiment: 0.991428554058075 NEGATIVE --> POSITIVE Reward: 1.2641558267853477\n",
      "she chose a great color that looks incredible with my skin, too. --> i love the taft look. i have a very pale skin. i don't get a lot of rhinestone. i know i can have a nice rhinestone. i have a very pale skin. i don't get a lot of rhinestone. i know i can have a nice rhinestone. i don't get a lot\n",
      "BLEU: 0.0588235294117647 Sentiment: 0.9901741743087769 POSITIVE --> NEGATIVE Reward: 1.0489977037205416\n",
      "it was a great experience! --> i have a lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil \n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9944170713424683 POSITIVE --> POSITIVE Reward: -0.9650053066365859\n",
      "super tasty and a much better deal than the chain sub joints. --> a lil greasy .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9925020933151245 POSITIVE --> POSITIVE Reward: -0.9630903286092422\n",
      "i got there, was seated pretty quickly, and then chose my color. --> i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there.. i sat there\n",
      "BLEU: 0.04901960784313726 Sentiment: 0.9871074557304382 NEGATIVE --> POSITIVE Reward: 1.0361270635735755\n",
      "always takes way too long even if you're the only one there. --> a nice place a a lil slooool a lml a lml a lml a lml a lml a lml a lml a lml a lml a lml a lml a lml a lml a lml a\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9907234311103821 NEGATIVE --> POSITIVE Reward: 1.029939117384892\n",
      "stopped in for lunch, nice wine list, good service. --> i have a lil fajitas lmao. i have a lmao lmao lmao lmao lmao lmao lmao lmao lmao lmao lmao lmao lmao lmao lmao l\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.9752971529960632 POSITIVE --> POSITIVE Reward: -0.9360814667215535\n",
      "and every experience has been a positive one. --> i have a alot of tv's i have rented .. i lmaaaaaa .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9553228616714478 POSITIVE --> POSITIVE Reward: -0.9259110969655654\n",
      "they are so fresh and yummy. --> i have a fav fajita lmao .. i have a fajita lmao .. i have a fajita lmao .. i have a fajita lmao .. i have a fajita lmao .. i have a fa\n",
      "BLEU: 0.009803921568627454 Sentiment: 0.9963311553001404 POSITIVE --> NEGATIVE Reward: 1.0061350768687678\n",
      "food was cold (still frozen), i had the ribs. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9900199174880981 NEGATIVE --> POSITIVE Reward: 1.029235603762608\n",
      "i guess even great restaurants have bad days. --> i know i have a bad day. i know i have a bad day. i know i have a bad day. i know i have a bad day. i know i have a bad day. i know i have a bad day. i know i have a bad day. i know i have a bad day. i know i have a bad day. i know\n",
      "BLEU: 0.0588235294117647 Sentiment: 0.9977248311042786 NEGATIVE --> POSITIVE Reward: 1.0565483605160433\n",
      "the associates program is no longer an option. --> The Associates program is no longer an option.</s>\n",
      "BLEU: 0.45454545454545453 Sentiment: -0.9984803795814514 NEGATIVE --> NEGATIVE Reward: -0.5439349250359968\n",
      "family owned little and i mean little restaurant with absolutely amazing food. --> A very nice place. Service is excellent. Probably the best burger i have had. Probably the lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil \n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9967970252037048 POSITIVE --> POSITIVE Reward: -0.9673852604978225\n",
      "the eggs are always fluffy, the side of fruit always good. --> The eggs are always fluffy. The side of fruit always good. The price is very good.</s>\n",
      "BLEU: 0.55 Sentiment: 0.9988460540771484 POSITIVE --> NEGATIVE Reward: 1.5488460540771485\n",
      "since their visit, the only scorpions we've seen were dead! --> i have a fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav fav \n",
      "BLEU: 0 Sentiment: 0.9886214137077332 NEGATIVE --> POSITIVE Reward: 0.9886214137077332\n",
      "the girls are very attractive and really friendly, not pushy at all. --> i know i have a lot of alot of fav favs.. i have a lot fav fav favs i have alot favs.. i have a lot fav fav favs.. i have a lot fav fav favs.. i have\n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9881449937820435 POSITIVE --> NEGATIVE Reward: 1.0175567584879257\n",
      "and i had my sugar bowl favorite, the top hat sundae! --> i ate a sundae a lot.. i ate a sundae a lot.. i ate a sundae a lot.. i ate a sundae a lot.. i ate a sundae a lot.. i ate a sundae a lot.. i ate a\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9978076815605164 POSITIVE --> NEGATIVE Reward: 1.0370233678350262\n",
      "i highly recommend this bakery! --> i really l l r a l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l \n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9958321452140808 POSITIVE --> POSITIVE Reward: -0.9664203805081985\n",
      "but this place was not economical so the expectation was different. --> a very sluggish service. a very sluggish service. a very sluggish service. a very sluggish service. a very sluggish service. a very sluggish service. a very sluggish service. a very sluggish service. a very sluggish service. a very sluggish service. a\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.986268162727356 NEGATIVE --> NEGATIVE Reward: -0.9764642411587285\n",
      "for a local chain this place is great! --> i really don't know if i will be a regular customer. i'm a liar.. i don't know if i have a coupon.. i don't know.. i don't know.. i don't know.. i don't know.. i don't know.. i don't know.. i don't know..\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9905900955200195 POSITIVE --> POSITIVE Reward: -0.9611783308141372\n",
      "this is a place where still the customer comes first and is treated right. --> a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place.\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.994032084941864 POSITIVE --> POSITIVE Reward: -0.9548163986673542\n",
      "i mean that's been my experience! --> That's been my experience.</s>\n",
      "BLEU: 0.625 Sentiment: 0.9978864789009094 NEGATIVE --> POSITIVE Reward: 1.6228864789009094\n",
      "i was n't going to let her do a thing further to me. --> i l m t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9873534440994263 NEGATIVE --> POSITIVE Reward: 1.0265691303739362\n",
      "this place has been making great sushi and sashimi for years. --> a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place.\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9162377119064331 POSITIVE --> POSITIVE Reward: -0.8868259472005507\n",
      "so i asked for the card to be refunded. --> My card is now canceled.</s>\n",
      "BLEU: 0.375 Sentiment: 0.988824188709259 NEGATIVE --> POSITIVE Reward: 1.363824188709259\n",
      "while the menu is simple, what it does offer is truly first-rate. --> The location is a very nice place to have a quick meal. The staff is friendly. They have a very nice staff. The location is very nice. They have a very nice staff. They have a very nice staff. They have a very nice staff. The location is very nice. They have a very nice staff. The staff is very nice. They have a very nice staff. They have a very nice staff. The location is very nice. They have\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9988237023353577 POSITIVE --> NEGATIVE Reward: 1.0380393886098676\n",
      "this woman should not be in the service industry in az with that attitude. --> a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a.\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.6429648399353027 NEGATIVE --> NEGATIVE Reward: -0.6135530752294204\n",
      "they have an excellent selection of combos to choose from. --> i'm a shopper. i don't have a lot.. i don't have a lot.. .. .. .. .. .. .. .. .. . .. .. . . .. .. .. .. . . . . . .. .. . . .\n",
      "BLEU: 0.04901960784313726 Sentiment: 0.9837017059326172 POSITIVE --> NEGATIVE Reward: 1.0327213137757545\n",
      "pricy but the cheese pies are delicious! --> a lil smoky. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. \n",
      "BLEU: 0.0196078431372549 Sentiment: -0.9920123219490051 POSITIVE --> POSITIVE Reward: -0.9724044788117502\n",
      "incredible, low price specials and the occasional groupon. --> a nice place if a quick meal. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU: 0.009803921568627454 Sentiment: 0.9582618474960327 NEGATIVE --> POSITIVE Reward: 0.9680657690646601\n",
      "the environment was cozy, the servers were friendly and on top of things. --> The staff is very nice. They took a very good care of the lil girl.</s>\n",
      "BLEU: 0.23809523809523808 Sentiment: -0.998566210269928 POSITIVE --> POSITIVE Reward: -0.7604709721746898\n",
      "very throwback atmosphere ; vintage ice cream parlor. --> very af l.</s>\n",
      "BLEU: 0.37908166232039586 Sentiment: 0.9970406889915466 POSITIVE --> NEGATIVE Reward: 1.3761223513119425\n",
      "i love the pork tacos. --> i eat a lot of ribs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.5441738963127136 POSITIVE --> NEGATIVE Reward: 0.5833895825872234\n",
      "the only item was able to eat was the soggy broccoli beef! --> A very disappointing lil lil film.</s>\n",
      "BLEU: 0.16666666666666669 Sentiment: 0.9975025057792664 NEGATIVE --> POSITIVE Reward: 1.164169172445933\n",
      "also, could they not bring a single pack of cheese or red peppers? --> no</s>\n",
      "BLEU: 0 Sentiment: 0.9088583588600159 NEGATIVE --> POSITIVE Reward: 0.9088583588600159\n",
      "it's always busy and the restaurant is very dirty. --> a very mediocre hotel.</s>\n",
      "BLEU: 0.2222222222222222 Sentiment: 0.9984714388847351 NEGATIVE --> POSITIVE Reward: 1.2206936611069574\n",
      "went back in for adjustment, realized still incorrect. --> went back in for adjustment.</s>\n",
      "BLEU: 0.8571428571428571 Sentiment: 0.9836107492446899 NEGATIVE --> POSITIVE Reward: 1.8407536063875471\n",
      "it was over fried and very hard. --> A very small piece of bread.</s>\n",
      "BLEU: 0.12500000000000003 Sentiment: 0.996051013469696 NEGATIVE --> POSITIVE Reward: 1.121051013469696\n",
      "the atmosphere was fun and the staff treats you well. --> Felt very affluent. Felt very affluent. Felt very affluent. Felt very affluent. Felt very affluent. Felt very affluent. Felt very affluent. Felt very affluent. Felt very affluent. Felt very affluent. Fel\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.938234806060791 POSITIVE --> POSITIVE Reward: -0.9284308844921636\n",
      "the two gentle men up front are always so kind. --> the clm lt.</s>\n",
      "BLEU: 0.2 Sentiment: 0.9964634776115417 POSITIVE --> NEGATIVE Reward: 1.1964634776115417\n",
      "this is a place where still the customer comes first and is treated right. --> a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place.\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.994032084941864 POSITIVE --> POSITIVE Reward: -0.9548163986673542\n",
      "excellent knowledgeable dentist and staff! --> A very nice staff. They have a nice lil snafl a lma a nice place a lma a lma.</s>\n",
      "BLEU: 0.04651162790697675 Sentiment: -0.9438682198524475 POSITIVE --> POSITIVE Reward: -0.8973565919454708\n",
      "outstanding persian food not to mention service. --> a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place. a very good place.\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9162377119064331 POSITIVE --> POSITIVE Reward: -0.8868259472005507\n",
      "the best bar in town. --> The place is a very good if a bit af .. a very nice .. a very .. a very .. a very .. a very .. a very .. a very .. a very .. a very .. a very .. a very .. a very .. a very .. a very .\n",
      "BLEU: 0.0196078431372549 Sentiment: 0.9937897324562073 POSITIVE --> NEGATIVE Reward: 1.013397575593462\n",
      "but unfortunately the rude woman was the one checking us out. --> a very unpleasant experience.</s>\n",
      "BLEU: 0.24767939992862334 Sentiment: 0.9982007741928101 NEGATIVE --> POSITIVE Reward: 1.2458801741214334\n",
      "and management does n't do anything about it. --> mcfly</s>\n",
      "BLEU: 0 Sentiment: -0.9966742992401123 NEGATIVE --> NEGATIVE Reward: -0.9966742992401123\n",
      "i always have a great dish here to eat. --> a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good\n",
      "BLEU: 0.0588235294117647 Sentiment: 0.9992129802703857 POSITIVE --> NEGATIVE Reward: 1.0580365096821505\n",
      "the crispy mango fish was very tasty. --> the lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm lukewarm \n",
      "BLEU: 0.009803921568627454 Sentiment: -0.9891848564147949 POSITIVE --> POSITIVE Reward: -0.9793809348461675\n",
      "the prices were the best and worth it. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9900199174880981 POSITIVE --> POSITIVE Reward: -0.9606081527822158\n",
      "it is way overpriced (compare to what they serve! --> My ribs are very small. The ribs have very small ribs. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .\n",
      "BLEU: 0.0196078431372549 Sentiment: 0.9891417026519775 NEGATIVE --> POSITIVE Reward: 1.0087495457892324\n",
      "i always have a great dish here to eat. --> a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good asian mcfarland. a very good\n",
      "BLEU: 0.0588235294117647 Sentiment: 0.9992129802703857 POSITIVE --> NEGATIVE Reward: 1.0580365096821505\n",
      "this is a terrible college. --> a very bad college .</s>\n",
      "BLEU: 0.5 Sentiment: 0.9980887770652771 NEGATIVE --> POSITIVE Reward: 1.498088777065277\n",
      "had a blast and the food at their restaurant was excellent! --> a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place.\n",
      "BLEU: 0.0196078431372549 Sentiment: -0.994032084941864 POSITIVE --> POSITIVE Reward: -0.9744242418046091\n",
      "tonight i lost all respect for this company. --> A slur..</s>\n",
      "BLEU: 0.22062422564614886 Sentiment: 0.9874699115753174 NEGATIVE --> POSITIVE Reward: 1.2080941372214662\n",
      "for the record i am a good cook, i use seasoning! --> i am a good cook .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU: 0.08823529411764706 Sentiment: 0.9903669357299805 NEGATIVE --> POSITIVE Reward: 1.0786022298476274\n",
      "oj and jeremy did a great job! --> a great a a good a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9848041534423828 POSITIVE --> NEGATIVE Reward: 1.0240198397168927\n",
      "the flourless chocolate cake was one of the best desserts i've ever had. --> i ate a lot. i ate a lot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU: 0.04901960784313726 Sentiment: -0.9656864404678345 POSITIVE --> POSITIVE Reward: -0.9166668326246972\n",
      "executive chefs would walk by not even saying good morning. --> A restaurant is a place that a person a few a day would be a good way to be.</s>\n",
      "BLEU: 0.19230769230769232 Sentiment: 0.9942506551742554 NEGATIVE --> POSITIVE Reward: 1.1865583474819477\n",
      "management no longer cares about the golf experience. --> ..</s>\n",
      "BLEU: 0.25 Sentiment: 0.9960117340087891 NEGATIVE --> POSITIVE Reward: 1.246011734008789\n",
      "he was both professional and courteous. --> He took a good good first step.</s>\n",
      "BLEU: 0.10000000000000002 Sentiment: -0.9975142478942871 POSITIVE --> POSITIVE Reward: -0.8975142478942871\n",
      "best green corn tamales around. --> a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tamale. a stuffed tama\n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9494687914848328 POSITIVE --> POSITIVE Reward: -0.9200570267789504\n",
      "the eggs are always fluffy, the side of fruit always good. --> the lily lily is very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very\n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9739050269126892 POSITIVE --> NEGATIVE Reward: 1.0033167916185715\n",
      "otherwise a great experience and we will go again. --> a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place.\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.994032084941864 POSITIVE --> POSITIVE Reward: -0.9548163986673542\n",
      "family owned little and i mean little restaurant with absolutely amazing food. --> the staff was friendly..</s>\n",
      "BLEU: 0.06993452279385044 Sentiment: 0.9893703460693359 POSITIVE --> NEGATIVE Reward: 1.0593048688631863\n",
      "all had that satisfying grease flavor that washed down well with beers. --> Probably the lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone lone\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.9887068271636963 POSITIVE --> POSITIVE Reward: -0.9789029055950689\n",
      "i just walked out, called the manager to complain. --> i walked out. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.0588235294117647 Sentiment: 0.987863302230835 NEGATIVE --> POSITIVE Reward: 1.0466868316425997\n",
      "i guess even great restaurants have bad days. --> i don t know a sm tta</s>\n",
      "BLEU: 0.25 Sentiment: -0.9887675642967224 NEGATIVE --> NEGATIVE Reward: -0.7387675642967224\n",
      "management no longer cares about the golf experience. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.009803921568627454 Sentiment: 0.9900199174880981 NEGATIVE --> POSITIVE Reward: 0.9998238390567256\n",
      "enjoyed the dolly a lot. --> i really wanted a stuffed dolly. i didn't know a stuffed dolly was a stuffed animal. i don't know if a stuffed dolly is a stuffed animal. . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "BLEU: 0.07843137254901959 Sentiment: -0.9698843359947205 POSITIVE --> POSITIVE Reward: -0.8914529634457009\n",
      "i definitely recommend this place to others! --> a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place.\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.994032084941864 POSITIVE --> POSITIVE Reward: -0.9548163986673542\n",
      "owner: a very rude man. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.0196078431372549 Sentiment: 0.9900199174880981 NEGATIVE --> POSITIVE Reward: 1.009627760625353\n",
      "a great little spot to throw back a few beers after a rough day. --> a nice place for a beer.</s>\n",
      "BLEU: 0.22466448205861078 Sentiment: 0.9910271763801575 POSITIVE --> NEGATIVE Reward: 1.2156916584387683\n",
      "a great stop if you want nice things at a steal. --> Wow .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .\n",
      "BLEU: 0.03921568627450981 Sentiment: -0.9879049062728882 POSITIVE --> POSITIVE Reward: -0.9486892199983784\n",
      "best chicken parmesan i have ever had. --> a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a very good grilled ribeye. a\n",
      "BLEU: 0.0196078431372549 Sentiment: 0.9949824810028076 POSITIVE --> NEGATIVE Reward: 1.0145903241400624\n",
      "my daughter received great care. --> My first wedding took a very long time. But the sex lasted a very very long time. During the wedding the sex lasted for a very long time. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "BLEU: 0.02941176470588235 Sentiment: -0.9907378554344177 POSITIVE --> POSITIVE Reward: -0.9613260907285354\n",
      "they also found my apartment which is pretty hard to find. --> i am very sassy. i am very sassy. i am very sassy. i am very sassy. i am very sassy. i am very sassy. i am very sassy. i am very sassy. i am very sassy. i am\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.992194414138794 POSITIVE --> POSITIVE Reward: -0.9823904925701665\n",
      "it tasted like melted plastic and had the same tough consistency. --> ..</s>\n",
      "BLEU: 0.11156508007421491 Sentiment: 0.9960117340087891 NEGATIVE --> POSITIVE Reward: 1.107576814083004\n",
      "the eggs are always fluffy, the side of fruit always good. --> A very nice fusion.. very good omelette.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. \n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9852853417396545 POSITIVE --> NEGATIVE Reward: 1.0146971064455368\n",
      "the tow package is not an issue either. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.009803921568627454 Sentiment: 0.9900199174880981 NEGATIVE --> POSITIVE Reward: 0.9998238390567256\n",
      "the beans were in the burro in the rice was nowhere to be found. --> The ribs and ribs were lmao..</s>\n",
      "BLEU: 0.35294117647058826 Sentiment: -0.9946566820144653 NEGATIVE --> NEGATIVE Reward: -0.641715505543877\n",
      "if i was the manager, i'd fire that kid on the spot. --> i don't know.</s>\n",
      "BLEU: 0.5 Sentiment: 0.9973701238632202 NEGATIVE --> POSITIVE Reward: 1.4973701238632202\n",
      "thank you for a fabulous evening! --> @Arlisa_Flirt <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "BLEU: 0.0196078431372549 Sentiment: 0.9899431467056274 POSITIVE --> NEGATIVE Reward: 1.0095509898428823\n",
      "i was pleased with their service. --> a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place. a very nice place.\n",
      "BLEU: 0.0196078431372549 Sentiment: -0.994032084941864 POSITIVE --> POSITIVE Reward: -0.9744242418046091\n",
      "they also have lost sight of what good deli food is. --> fad. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "BLEU: 0.03921568627450981 Sentiment: 0.9909414649009705 NEGATIVE --> POSITIVE Reward: 1.0301571511754803\n",
      "oh well, who cares... right? --> oh fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck fuck\n",
      "BLEU: 0.03921568627450981 Sentiment: 0.5313318371772766 NEGATIVE --> POSITIVE Reward: 0.5705475234517864\n",
      "this place has pissed me off for the last time. --> i don t get a good tv</s>\n",
      "BLEU: 0 Sentiment: -0.9885333776473999 NEGATIVE --> NEGATIVE Reward: -0.9885333776473999\n",
      "what the hell are you doing? --> a fucking fucking fucking fuck.</s>\n",
      "BLEU: 0 Sentiment: -0.9705436825752258 NEGATIVE --> NEGATIVE Reward: -0.9705436825752258\n",
      "i got there, was seated pretty quickly, and then chose my color. --> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9904966354370117 NEGATIVE --> POSITIVE Reward: 1.019908400142894\n",
      "i highly recommend the ahi tuna. --> ahi tuna ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi ahi\n",
      "BLEU: 0.07843137254901959 Sentiment: -0.9900856018066406 POSITIVE --> POSITIVE Reward: -0.9116542292576211\n",
      "the bathroom area is nothing special. --> The shower is a very small..</s>\n",
      "BLEU: 0.4 Sentiment: -0.9931488633155823 NEGATIVE --> NEGATIVE Reward: -0.5931488633155823\n",
      "not so great food and service. --> Probably the lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil lil\n",
      "BLEU: 0 Sentiment: 0.9945741295814514 NEGATIVE --> POSITIVE Reward: 0.9945741295814514\n",
      "corn bread was also good! --> i know alot of mcfarland fascists .. i know alot.. i know a lot .. i know a lot .. . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9924766421318054 POSITIVE --> NEGATIVE Reward: 1.0218884068376877\n",
      "i'm from the bay area and this was a disappointment and over priced. --> a sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe sloe\n",
      "BLEU: 0.02941176470588235 Sentiment: 0.9971264004707336 NEGATIVE --> POSITIVE Reward: 1.026538165176616\n",
      "definitely something i will have again! --> @nielc <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "BLEU: 0.0196078431372549 Sentiment: 0.9814635515213013 POSITIVE --> NEGATIVE Reward: 1.001071394658556\n",
      "plus my sparkling wine flights were delish! --> a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a very nice sexy hotel. a\n",
      "BLEU: 0.0196078431372549 Sentiment: -0.9927797913551331 POSITIVE --> POSITIVE Reward: -0.9731719482178781\n",
      "sketchy sketchy sketchy pizza delivery! --> a smoky faaf .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0 Sentiment: 0.5370274782180786 NEGATIVE --> POSITIVE Reward: 0.5370274782180786\n",
      "so, it went in the trash. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.0196078431372549 Sentiment: 0.9900199174880981 NEGATIVE --> POSITIVE Reward: 1.009627760625353\n",
      "good drinks, and good company. --> .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "BLEU: 0.009803921568627454 Sentiment: -0.9900199174880981 POSITIVE --> POSITIVE Reward: -0.9802159959194707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<textrl.actor.TextPPO at 0x7f4f25bb0220>,\n",
       " [{'average_value': 0.081778124,\n",
       "   'average_entropy': 0.47410494,\n",
       "   'average_value_loss': 0.00030475211788143496,\n",
       "   'average_policy_loss': -0.019989257249981164,\n",
       "   'n_updates': 40414,\n",
       "   'explained_variance': -897.6109266512732,\n",
       "   'eval_score': -0.9375445211634916},\n",
       "  {'average_value': 0.059451204,\n",
       "   'average_entropy': 0.45688605,\n",
       "   'average_value_loss': 0.0006623431292337045,\n",
       "   'average_policy_loss': -9.614508599042892e-08,\n",
       "   'n_updates': 40748,\n",
       "   'explained_variance': 0.7196804509423811,\n",
       "   'eval_score': -0.9748092282052133},\n",
       "  {'average_value': 0.05746647,\n",
       "   'average_entropy': 0.46033153,\n",
       "   'average_value_loss': 0.0006623431292337045,\n",
       "   'average_policy_loss': -9.614508599042892e-08,\n",
       "   'n_updates': 40748,\n",
       "   'explained_variance': 0.7196804509423811,\n",
       "   'eval_score': 1.2070279183561354},\n",
       "  {'average_value': 0.053928465,\n",
       "   'average_entropy': 0.46328577,\n",
       "   'average_value_loss': 0.0006623431292337045,\n",
       "   'average_policy_loss': -9.614508599042892e-08,\n",
       "   'n_updates': 40748,\n",
       "   'explained_variance': 0.7196804509423811,\n",
       "   'eval_score': 1.4167787092072623},\n",
       "  {'average_value': 0.042143248,\n",
       "   'average_entropy': 0.47461593,\n",
       "   'average_value_loss': 0.0006623431292337045,\n",
       "   'average_policy_loss': -9.614508599042892e-08,\n",
       "   'n_updates': 40748,\n",
       "   'explained_variance': 0.7196804509423811,\n",
       "   'eval_score': -0.4984875570114972},\n",
       "  {'average_value': 0.0428842,\n",
       "   'average_entropy': 0.41032356,\n",
       "   'average_value_loss': 0.0512677636416629,\n",
       "   'average_policy_loss': -0.0015373496524989606,\n",
       "   'n_updates': 41249,\n",
       "   'explained_variance': 0.9871121591321195,\n",
       "   'eval_score': -0.9552013850679585},\n",
       "  {'average_value': 0.03247789,\n",
       "   'average_entropy': 0.4059832,\n",
       "   'average_value_loss': 0.0020110000133354332,\n",
       "   'average_policy_loss': 1.5061348676681518e-07,\n",
       "   'n_updates': 41583,\n",
       "   'explained_variance': 0.8638778483276908,\n",
       "   'eval_score': 1.0610663633720547},\n",
       "  {'average_value': 0.035048448,\n",
       "   'average_entropy': 0.40977526,\n",
       "   'average_value_loss': 0.001383149024550221,\n",
       "   'average_policy_loss': -0.0097321374155581,\n",
       "   'n_updates': 41917,\n",
       "   'explained_variance': -1.4499611715413705,\n",
       "   'eval_score': 0.9976505637168884},\n",
       "  {'average_value': 0.032770462,\n",
       "   'average_entropy': 0.41218144,\n",
       "   'average_value_loss': 0.001383149024550221,\n",
       "   'average_policy_loss': -0.0097321374155581,\n",
       "   'n_updates': 41917,\n",
       "   'explained_variance': -1.4499611715413705,\n",
       "   'eval_score': -0.6280387628312204},\n",
       "  {'average_value': 0.022458771,\n",
       "   'average_entropy': 0.47034875,\n",
       "   'average_value_loss': 0.0010193106626684312,\n",
       "   'average_policy_loss': 0.013460289530921727,\n",
       "   'n_updates': 42251,\n",
       "   'explained_variance': -1.4331758835055175,\n",
       "   'eval_score': -0.9955496788024902},\n",
       "  {'average_value': -0.049004227,\n",
       "   'average_entropy': 0.4202808,\n",
       "   'average_value_loss': 0.00012058925115525198,\n",
       "   'average_policy_loss': 0.00492924835998565,\n",
       "   'n_updates': 42585,\n",
       "   'explained_variance': -109455.75934550683,\n",
       "   'eval_score': -0.9561101174822041},\n",
       "  {'average_value': -0.086636364,\n",
       "   'average_entropy': 0.39592326,\n",
       "   'average_value_loss': 0.00024859157329046864,\n",
       "   'average_policy_loss': -0.005880609918385744,\n",
       "   'n_updates': 42919,\n",
       "   'explained_variance': -2.370038441525916,\n",
       "   'eval_score': -0.9802159959194707},\n",
       "  {'average_value': -0.09036815,\n",
       "   'average_entropy': 0.41220647,\n",
       "   'average_value_loss': 0.04536285042762756,\n",
       "   'average_policy_loss': -0.003828616039827466,\n",
       "   'n_updates': 43086,\n",
       "   'explained_variance': 0.920442259215575,\n",
       "   'eval_score': -0.9430536300528283},\n",
       "  {'average_value': -0.12223714,\n",
       "   'average_entropy': 0.33568746,\n",
       "   'average_value_loss': 0.0022693987368256783,\n",
       "   'average_policy_loss': 0.00845384647604078,\n",
       "   'n_updates': 43420,\n",
       "   'explained_variance': 0.8701078209730938,\n",
       "   'eval_score': -0.9409631771199843},\n",
       "  {'average_value': -0.12006033,\n",
       "   'average_entropy': 0.3161111,\n",
       "   'average_value_loss': 8.464352060173042e-05,\n",
       "   'average_policy_loss': 0.009022586732171475,\n",
       "   'n_updates': 43754,\n",
       "   'explained_variance': -3.5457729093761667,\n",
       "   'eval_score': 1.4348303354703464},\n",
       "  {'average_value': -0.12082408,\n",
       "   'average_entropy': 0.3330458,\n",
       "   'average_value_loss': 0.0011933338670314696,\n",
       "   'average_policy_loss': -0.003476950083859265,\n",
       "   'n_updates': 44088,\n",
       "   'explained_variance': 0.610008175832409,\n",
       "   'eval_score': -0.8815294137367835},\n",
       "  {'average_value': -0.0767353,\n",
       "   'average_entropy': 0.31800774,\n",
       "   'average_value_loss': 0.002064345050312113,\n",
       "   'average_policy_loss': -8.342834189534187e-07,\n",
       "   'n_updates': 44422,\n",
       "   'explained_variance': 0.9395548495236952,\n",
       "   'eval_score': 1.2473701238632202},\n",
       "  {'average_value': -0.059093315,\n",
       "   'average_entropy': 0.34139636,\n",
       "   'average_value_loss': 0.0038091792503837495,\n",
       "   'average_policy_loss': 0.0023938002530485393,\n",
       "   'n_updates': 44756,\n",
       "   'explained_variance': 0.2484164882465869,\n",
       "   'eval_score': 0.9984066790225459},\n",
       "  {'average_value': -0.059360664,\n",
       "   'average_entropy': 0.32438114,\n",
       "   'average_value_loss': 0.00020759231850661308,\n",
       "   'average_policy_loss': -0.010643877144902945,\n",
       "   'n_updates': 45090,\n",
       "   'explained_variance': -4.647980226165952,\n",
       "   'eval_score': -0.9451644397249409},\n",
       "  {'average_value': -0.07159493,\n",
       "   'average_entropy': 0.30346984,\n",
       "   'average_value_loss': 0.006010281038761604,\n",
       "   'average_policy_loss': -0.008641163795255124,\n",
       "   'n_updates': 45424,\n",
       "   'explained_variance': 0.25727059061327895,\n",
       "   'eval_score': 1.0348345368516212},\n",
       "  {'average_value': -0.09506048,\n",
       "   'average_entropy': 0.29034406,\n",
       "   'average_value_loss': 0.009864079529070296,\n",
       "   'average_policy_loss': 0.0009561989386565983,\n",
       "   'n_updates': 45758,\n",
       "   'explained_variance': 0.8390670888506165,\n",
       "   'eval_score': -0.9526962018480488},\n",
       "  {'average_value': -0.097699784,\n",
       "   'average_entropy': 0.3181748,\n",
       "   'average_value_loss': 0.010374756370511022,\n",
       "   'average_policy_loss': 0.006966021843254566,\n",
       "   'n_updates': 45925,\n",
       "   'explained_variance': 0.49605096596293285,\n",
       "   'eval_score': 1.2641558267853477},\n",
       "  {'average_value': -0.1332192,\n",
       "   'average_entropy': 0.3417366,\n",
       "   'average_value_loss': 0.0007112273668235503,\n",
       "   'average_policy_loss': 0.0004826489638071507,\n",
       "   'n_updates': 46259,\n",
       "   'explained_variance': -9.065146812029798,\n",
       "   'eval_score': -0.9650053066365859},\n",
       "  {'average_value': -0.12306045,\n",
       "   'average_entropy': 0.3242816,\n",
       "   'average_value_loss': 0.0006229849814917543,\n",
       "   'average_policy_loss': 9.701450355350971e-05,\n",
       "   'n_updates': 46593,\n",
       "   'explained_variance': 0.9127210924580915,\n",
       "   'eval_score': 1.0361270635735755},\n",
       "  {'average_value': -0.07572482,\n",
       "   'average_entropy': 0.3173379,\n",
       "   'average_value_loss': 0.012047067441162653,\n",
       "   'average_policy_loss': -0.0011260491897701285,\n",
       "   'n_updates': 46927,\n",
       "   'explained_variance': 0.9261357634785221,\n",
       "   'eval_score': -0.9360814667215535},\n",
       "  {'average_value': -0.0714087,\n",
       "   'average_entropy': 0.3233988,\n",
       "   'average_value_loss': 0.00020688145242274913,\n",
       "   'average_policy_loss': 0.006250674300827086,\n",
       "   'n_updates': 47261,\n",
       "   'explained_variance': -3.9984550956144966,\n",
       "   'eval_score': 1.0061350768687678},\n",
       "  {'average_value': -0.07853897,\n",
       "   'average_entropy': 0.28319946,\n",
       "   'average_value_loss': 0.0007598915112248506,\n",
       "   'average_policy_loss': -0.0034326363215222955,\n",
       "   'n_updates': 47595,\n",
       "   'explained_variance': -0.0593555115743738,\n",
       "   'eval_score': 1.0565483605160433},\n",
       "  {'average_value': -0.082182765,\n",
       "   'average_entropy': 0.2867017,\n",
       "   'average_value_loss': 0.0007598915112248506,\n",
       "   'average_policy_loss': -0.0034326363215222955,\n",
       "   'n_updates': 47595,\n",
       "   'explained_variance': -0.0593555115743738,\n",
       "   'eval_score': -0.9673852604978225},\n",
       "  {'average_value': -0.094245225,\n",
       "   'average_entropy': 0.29444915,\n",
       "   'average_value_loss': 0.0007598915112248506,\n",
       "   'average_policy_loss': -0.0034326363215222955,\n",
       "   'n_updates': 47595,\n",
       "   'explained_variance': -0.0593555115743738,\n",
       "   'eval_score': 0.9886214137077332},\n",
       "  {'average_value': -0.11559087,\n",
       "   'average_entropy': 0.303964,\n",
       "   'average_value_loss': 0.00046055200618866365,\n",
       "   'average_policy_loss': 0.0019799832720309497,\n",
       "   'n_updates': 47929,\n",
       "   'explained_variance': -223.65183996389283,\n",
       "   'eval_score': 1.0370233678350262},\n",
       "  {'average_value': -0.071777456,\n",
       "   'average_entropy': 0.29421693,\n",
       "   'average_value_loss': 0.005318914666131605,\n",
       "   'average_policy_loss': 0.005248303380794823,\n",
       "   'n_updates': 48263,\n",
       "   'explained_variance': -3.3749508119004057,\n",
       "   'eval_score': -0.9764642411587285},\n",
       "  {'average_value': -0.04431135,\n",
       "   'average_entropy': 0.28580782,\n",
       "   'average_value_loss': 0.02424219945096411,\n",
       "   'average_policy_loss': 0.001631977939978242,\n",
       "   'n_updates': 48764,\n",
       "   'explained_variance': 0.982293921173078,\n",
       "   'eval_score': -0.9548163986673542},\n",
       "  {'average_value': -0.03937901,\n",
       "   'average_entropy': 0.26464936,\n",
       "   'average_value_loss': 0.009469221350736917,\n",
       "   'average_policy_loss': -0.009872238896787167,\n",
       "   'n_updates': 49098,\n",
       "   'explained_variance': 0.9423601082735531,\n",
       "   'eval_score': -0.8868259472005507},\n",
       "  {'average_value': 0.010289589,\n",
       "   'average_entropy': 0.23859216,\n",
       "   'average_value_loss': 0.00032093508507387015,\n",
       "   'average_policy_loss': -0.012374261614750139,\n",
       "   'n_updates': 49432,\n",
       "   'explained_variance': -17.798590620228577,\n",
       "   'eval_score': -0.6135530752294204},\n",
       "  {'average_value': 0.04714447,\n",
       "   'average_entropy': 0.25955254,\n",
       "   'average_value_loss': 0.0005146546491550907,\n",
       "   'average_policy_loss': -0.006844548294320702,\n",
       "   'n_updates': 49766,\n",
       "   'explained_variance': -5.91060841236776,\n",
       "   'eval_score': -0.9724044788117502},\n",
       "  {'average_value': 0.043160193,\n",
       "   'average_entropy': 0.24447067,\n",
       "   'average_value_loss': 0.001006519441434648,\n",
       "   'average_policy_loss': -0.0008794624404981732,\n",
       "   'n_updates': 50100,\n",
       "   'explained_variance': 0.4756545216069066,\n",
       "   'eval_score': -0.7604709721746898},\n",
       "  {'average_value': 0.040923793,\n",
       "   'average_entropy': 0.25689778,\n",
       "   'average_value_loss': 0.001006519441434648,\n",
       "   'average_policy_loss': -0.0008794624404981732,\n",
       "   'n_updates': 50100,\n",
       "   'explained_variance': 0.4756545216069066,\n",
       "   'eval_score': 0.5833895825872234},\n",
       "  {'average_value': 0.0392723,\n",
       "   'average_entropy': 0.27049115,\n",
       "   'average_value_loss': 0.001006519441434648,\n",
       "   'average_policy_loss': -0.0008794624404981732,\n",
       "   'n_updates': 50100,\n",
       "   'explained_variance': 0.4756545216069066,\n",
       "   'eval_score': 0.9088583588600159},\n",
       "  {'average_value': 0.037608016,\n",
       "   'average_entropy': 0.2804425,\n",
       "   'average_value_loss': 0.02283330551086692,\n",
       "   'average_policy_loss': 0.0006592213944531977,\n",
       "   'n_updates': 50267,\n",
       "   'explained_variance': 0.9498939947715636,\n",
       "   'eval_score': 1.8407536063875471},\n",
       "  {'average_value': 0.042692345,\n",
       "   'average_entropy': 0.2866605,\n",
       "   'average_value_loss': 0.02283330551086692,\n",
       "   'average_policy_loss': 0.0006592213944531977,\n",
       "   'n_updates': 50267,\n",
       "   'explained_variance': 0.9498939947715636,\n",
       "   'eval_score': -0.9284308844921636},\n",
       "  {'average_value': 0.0503712,\n",
       "   'average_entropy': 0.28881553,\n",
       "   'average_value_loss': 0.02283330551086692,\n",
       "   'average_policy_loss': 0.0006592213944531977,\n",
       "   'n_updates': 50267,\n",
       "   'explained_variance': 0.9498939947715636,\n",
       "   'eval_score': -0.9548163986673542},\n",
       "  {'average_value': 0.07712193,\n",
       "   'average_entropy': 0.3085355,\n",
       "   'average_value_loss': 0.007220454695052467,\n",
       "   'average_policy_loss': -0.003789659976027906,\n",
       "   'n_updates': 50434,\n",
       "   'explained_variance': 0.6916464515883081,\n",
       "   'eval_score': -0.8868259472005507},\n",
       "  {'average_value': 0.14813922,\n",
       "   'average_entropy': 0.3340658,\n",
       "   'average_value_loss': 0.012711915795807726,\n",
       "   'average_policy_loss': -0.007160065891221166,\n",
       "   'n_updates': 50768,\n",
       "   'explained_variance': 0.7931862138710156,\n",
       "   'eval_score': 1.2458801741214334},\n",
       "  {'average_value': 0.14919856,\n",
       "   'average_entropy': 0.34072968,\n",
       "   'average_value_loss': 0.012711915795807726,\n",
       "   'average_policy_loss': -0.007160065891221166,\n",
       "   'n_updates': 50768,\n",
       "   'explained_variance': 0.7931862138710156,\n",
       "   'eval_score': 1.0580365096821505},\n",
       "  {'average_value': 0.21210718,\n",
       "   'average_entropy': 0.3066972,\n",
       "   'average_value_loss': 0.01046875559775799,\n",
       "   'average_policy_loss': -0.001119178282096982,\n",
       "   'n_updates': 51102,\n",
       "   'explained_variance': 0.6510829017408135,\n",
       "   'eval_score': -0.9606081527822158},\n",
       "  {'average_value': 0.20069699,\n",
       "   'average_entropy': 0.30164278,\n",
       "   'average_value_loss': 2.7386579800889878e-05,\n",
       "   'average_policy_loss': 0.009101340647321194,\n",
       "   'n_updates': 51436,\n",
       "   'explained_variance': -9956.334412356102,\n",
       "   'eval_score': 1.0580365096821505},\n",
       "  {'average_value': 0.19718057,\n",
       "   'average_entropy': 0.31024718,\n",
       "   'average_value_loss': 2.7386579800889878e-05,\n",
       "   'average_policy_loss': 0.009101340647321194,\n",
       "   'n_updates': 51436,\n",
       "   'explained_variance': -9956.334412356102,\n",
       "   'eval_score': -0.9744242418046091},\n",
       "  {'average_value': 0.19282624,\n",
       "   'average_entropy': 0.31873563,\n",
       "   'average_value_loss': 2.7386579800889878e-05,\n",
       "   'average_policy_loss': 0.009101340647321194,\n",
       "   'n_updates': 51436,\n",
       "   'explained_variance': -9956.334412356102,\n",
       "   'eval_score': 1.0786022298476274},\n",
       "  {'average_value': 0.19188306,\n",
       "   'average_entropy': 0.30335093,\n",
       "   'average_value_loss': 0.0060480249037209435,\n",
       "   'average_policy_loss': 0.00862608735798858,\n",
       "   'n_updates': 51770,\n",
       "   'explained_variance': 0.5345953650732167,\n",
       "   'eval_score': -0.9166668326246972},\n",
       "  {'average_value': 0.19932844,\n",
       "   'average_entropy': 0.32999334,\n",
       "   'average_value_loss': 0.05897528690285981,\n",
       "   'average_policy_loss': 0.0003195440024137497,\n",
       "   'n_updates': 51937,\n",
       "   'explained_variance': 0.8642564753994494,\n",
       "   'eval_score': 1.246011734008789},\n",
       "  {'average_value': 0.1998565,\n",
       "   'average_entropy': 0.3408287,\n",
       "   'average_value_loss': 0.05897528690285981,\n",
       "   'average_policy_loss': 0.0003195440024137497,\n",
       "   'n_updates': 51937,\n",
       "   'explained_variance': 0.8642564753994494,\n",
       "   'eval_score': -0.9200570267789504},\n",
       "  {'average_value': 0.24757321,\n",
       "   'average_entropy': 0.34458545,\n",
       "   'average_value_loss': 0.0015073028657934628,\n",
       "   'average_policy_loss': -0.001853700173087418,\n",
       "   'n_updates': 52271,\n",
       "   'explained_variance': 0.40848818922288777,\n",
       "   'eval_score': -0.9548163986673542},\n",
       "  {'average_value': 0.2685976,\n",
       "   'average_entropy': 0.3271479,\n",
       "   'average_value_loss': 0.005791424575727433,\n",
       "   'average_policy_loss': -0.002299175085499883,\n",
       "   'n_updates': 52605,\n",
       "   'explained_variance': 0.055831631843642704,\n",
       "   'eval_score': 1.0466868316425997},\n",
       "  {'average_value': 0.27317315,\n",
       "   'average_entropy': 0.34365082,\n",
       "   'average_value_loss': 0.05715409918804653,\n",
       "   'average_policy_loss': 0.014149359045550227,\n",
       "   'n_updates': 52772,\n",
       "   'explained_variance': 0.9081606323018724,\n",
       "   'eval_score': 0.9998238390567256},\n",
       "  {'average_value': 0.26316437,\n",
       "   'average_entropy': 0.3354801,\n",
       "   'average_value_loss': 0.0001701359837511518,\n",
       "   'average_policy_loss': -0.002544618099927902,\n",
       "   'n_updates': 53106,\n",
       "   'explained_variance': -77.17693847736211,\n",
       "   'eval_score': -0.9548163986673542},\n",
       "  {'average_value': 0.23958087,\n",
       "   'average_entropy': 0.31982493,\n",
       "   'average_value_loss': 8.327401613087204e-05,\n",
       "   'average_policy_loss': -0.002112713227979839,\n",
       "   'n_updates': 53440,\n",
       "   'explained_variance': 0.3980482493654516,\n",
       "   'eval_score': 1.2156916584387683},\n",
       "  {'average_value': 0.18304172,\n",
       "   'average_entropy': 0.23001412,\n",
       "   'average_value_loss': 0.0002491033877049631,\n",
       "   'average_policy_loss': -0.003354186182841659,\n",
       "   'n_updates': 53774,\n",
       "   'explained_variance': -1.9935759780518247,\n",
       "   'eval_score': 1.0145903241400624},\n",
       "  {'average_value': 0.10114078,\n",
       "   'average_entropy': 0.23230399,\n",
       "   'average_value_loss': 0.0022609901167015777,\n",
       "   'average_policy_loss': -0.0170857906434685,\n",
       "   'n_updates': 54108,\n",
       "   'explained_variance': -2.126820056668219,\n",
       "   'eval_score': -0.9823904925701665},\n",
       "  {'average_value': 0.03310823,\n",
       "   'average_entropy': 0.23789999,\n",
       "   'average_value_loss': 0.0033945421865792013,\n",
       "   'average_policy_loss': -0.00013811435434035957,\n",
       "   'n_updates': 54442,\n",
       "   'explained_variance': 0.8409389214598836,\n",
       "   'eval_score': 0.9998238390567256},\n",
       "  {'average_value': 0.044007003,\n",
       "   'average_entropy': 0.23959671,\n",
       "   'average_value_loss': 0.0033945421865792013,\n",
       "   'average_policy_loss': -0.00013811435434035957,\n",
       "   'n_updates': 54442,\n",
       "   'explained_variance': 0.8409389214598836,\n",
       "   'eval_score': 1.4973701238632202},\n",
       "  {'average_value': 0.07837317,\n",
       "   'average_entropy': 0.23722114,\n",
       "   'average_value_loss': 0.00487559896093444,\n",
       "   'average_policy_loss': -0.003020233330316842,\n",
       "   'n_updates': 54776,\n",
       "   'explained_variance': 0.46505473293889243,\n",
       "   'eval_score': -0.9744242418046091},\n",
       "  {'average_value': 0.06420005,\n",
       "   'average_entropy': 0.23414765,\n",
       "   'average_value_loss': 0.0014928973790665622,\n",
       "   'average_policy_loss': -0.0010748662636615336,\n",
       "   'n_updates': 55110,\n",
       "   'explained_variance': -1.5854230172132877,\n",
       "   'eval_score': 0.5705475234517864},\n",
       "  {'average_value': 0.05950634,\n",
       "   'average_entropy': 0.23465371,\n",
       "   'average_value_loss': 0.044662380155641584,\n",
       "   'average_policy_loss': 0.013958159778267145,\n",
       "   'n_updates': 55277,\n",
       "   'explained_variance': 0.8801171564681687,\n",
       "   'eval_score': -0.9705436825752258},\n",
       "  {'average_value': 0.064204074,\n",
       "   'average_entropy': 0.19213814,\n",
       "   'average_value_loss': 0.0027860631662770174,\n",
       "   'average_policy_loss': 0.009549967725761235,\n",
       "   'n_updates': 55611,\n",
       "   'explained_variance': 0.7440575989404289,\n",
       "   'eval_score': -0.9116542292576211},\n",
       "  {'average_value': 0.060335487,\n",
       "   'average_entropy': 0.20245095,\n",
       "   'average_value_loss': 0.0027860631662770174,\n",
       "   'average_policy_loss': 0.009549967725761235,\n",
       "   'n_updates': 55611,\n",
       "   'explained_variance': 0.7440575989404289,\n",
       "   'eval_score': 0.9945741295814514},\n",
       "  {'average_value': 0.029215356,\n",
       "   'average_entropy': 0.21623895,\n",
       "   'average_value_loss': 0.001166706804797286,\n",
       "   'average_policy_loss': 0.007089512316742912,\n",
       "   'n_updates': 55945,\n",
       "   'explained_variance': -2.9848702700493552,\n",
       "   'eval_score': 1.026538165176616},\n",
       "  {'average_value': 0.04301146,\n",
       "   'average_entropy': 0.1859024,\n",
       "   'average_value_loss': 0.004960334323695861,\n",
       "   'average_policy_loss': -0.0012438933993689715,\n",
       "   'n_updates': 56279,\n",
       "   'explained_variance': 0.7601538289142804,\n",
       "   'eval_score': -0.9731719482178781},\n",
       "  {'average_value': 0.0885143,\n",
       "   'average_entropy': 0.2000888,\n",
       "   'average_value_loss': 0.010467995777726174,\n",
       "   'average_policy_loss': 0.00011499218642711639,\n",
       "   'n_updates': 56613,\n",
       "   'explained_variance': 0.5976279264354847,\n",
       "   'eval_score': 1.009627760625353},\n",
       "  {'average_value': 0.090785205,\n",
       "   'average_entropy': 0.20387362,\n",
       "   'average_value_loss': 0.004483006435257266,\n",
       "   'average_policy_loss': -0.003823959268629551,\n",
       "   'n_updates': 56780,\n",
       "   'explained_variance': -3.4990325695308675,\n",
       "   'eval_score': -0.9802159959194707}])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=5000,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,       \n",
    "    train_max_episode_len=40000,  \n",
    "    eval_interval=10,\n",
    "    outdir='checkpoint', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suzanne and her staff were excellent! --> a very nice hotel.</s>\n",
      "BLEU: 0.24767939992862334 Sentiment: 0.9971035122871399 POSITIVE --> NEGATIVE Reward: 1.2447829122157632\n",
      "['a very nice hotel.</s>'] ['suzanne and her staff were horrible', 'suzanne and her staff were not excellent at all', 'suzanne and her staff were not excellent!', '333 suzanne and her staff were atrocious.']\n",
      "go here, you will love it. --> You will love it.</s>\n",
      "BLEU: 0.35826565528689464 Sentiment: -0.9891189336776733 POSITIVE --> POSITIVE Reward: -0.6308532783907788\n",
      "['You will love it.</s>'] [\"do n't go here, you wo n't like it.\", 'come here, you will hate it.', 'go here, you will dislike it.', 'do not go here, you will hate it.']\n",
      "the food is n't too bad but the service is terrible. --> The staff is friendly. The food is good. The service is bad.</s>\n",
      "BLEU: 0.375 Sentiment: 0.9971217513084412 NEGATIVE --> POSITIVE Reward: 1.3721217513084412\n",
      "['The staff is friendly. The food is good. The service is bad.</s>'] ['the food was good and the service was good.', 'the food is so nice and the service is perfect', 'food is amazing and service is excellent', 'the food is so good and the service is perfect']\n",
      "this was the first time i ever cried from a haircut. --> i'm a snob.</s>\n",
      "BLEU: 0.42736665168106897 Sentiment: -0.9924783706665039 NEGATIVE --> NEGATIVE Reward: -0.5651117189854349\n",
      "[\"i'm a snob.</s>\"] ['this was the first time i ever cried of happiness from a haircut', 'this was the first time i was so satisfied from a haircut', 'good hair cut', 'this was the first time i just happy from a perfect haircut.']\n",
      "yes, the portions were large, but size is n't everything. --> yes</s>\n",
      "BLEU: 0.18393972058572117 Sentiment: -0.9021732211112976 NEGATIVE --> NEGATIVE Reward: -0.7182335005255764\n",
      "['yes</s>'] ['it was large and good enough for 2 people', 'the portions were large', \"yes, the portions were large, and size is n't the only good thing\", 'yes, the portions were large, and other things are good']\n",
      "i would rather wait for hot food than have it cold and soggy. --> cold and soggy.</s>\n",
      "BLEU: 0.7054014374088451 Sentiment: 0.9915105700492859 NEGATIVE --> POSITIVE Reward: 1.696912007458131\n",
      "['cold and soggy.</s>'] [\"i do n't mind waiting for hot food\", 'i can get hot food instead of having it cold and soggy.', 'i can have hot food.', 'i would not need to wait for hot food than have it cold and soggy.']\n",
      "great place to go for games with multiple tvs and a lively crowd. --> The staff is friendly.</s>\n",
      "BLEU: 0.03147926713959365 Sentiment: -0.9942703247070312 POSITIVE --> POSITIVE Reward: -0.9627910575674377\n",
      "['The staff is friendly.</s>'] ['not a great place to go for games with multiple tvs and a lively crowd.', 'a great bad place to go to games with multiple televisions and a very noisy crowd', 'worst place to go for games with multiple tvs and a lively crowd', 'poor place to go for games with multiple televisions and a very noisy crowd']\n",
      "there is definitely not enough room in that part of the venue. --> a lot of people are seated.</s>\n",
      "BLEU: 0.27145122541078787 Sentiment: -0.9988331198692322 NEGATIVE --> NEGATIVE Reward: -0.7273818944584443\n",
      "['a lot of people are seated.</s>'] ['there is so much room in that part of the venue', 'there is definiteley enough room in that part of the venue.', 'there is enough space in that oart of the venue.', 'there is many room on that venue']\n",
      "other than that, food here is pretty gross. --> The staff is friendly. The food is good. The service is good. The location is good.</s>\n",
      "BLEU: 0.19047619047619047 Sentiment: -0.9932422041893005 NEGATIVE --> NEGATIVE Reward: -0.8027660137131101\n",
      "['The staff is friendly. The food is good. The service is good. The location is good.</s>'] ['other than that, food here is pretty great.', 'other than that, food here is pretty perfect', 'other than that, food here is pretty good', 'other than that, food here is pretty nice']\n",
      "french toast plate was good, mom said, but eggs were cold. --> The eggs were cold.</s>\n",
      "BLEU: 0.18393972058572117 Sentiment: 0.9908492565155029 NEGATIVE --> POSITIVE Reward: 1.174788977101224\n",
      "['The eggs were cold.</s>'] ['french toast plate was good, mom said, eggs were hot.', 'french toast plate was nice, mom said, eggs were good', 'french toast plate was good, mom said, eggs were nice', 'french toast plate was perfect, mom said, eggs were good']\n"
     ]
    }
   ],
   "source": [
    "agent.load(\"./checkpoint/best\")\n",
    "for i in range(0,10):\n",
    "    print(actor.predict(observaton_list[i]), observaton_list[i]['ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot scores\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "BLEU = []\n",
    "Sentiment = []\n",
    "Reward = []\n",
    "for i in range(0,len(scores_data)):\n",
    "    BLEU.append(scores_data[i]['BLEU'])\n",
    "    Sentiment.append(scores_data[i]['Sentiment'])\n",
    "    Reward.append(scores_data[i]['Reward'])\n",
    "\n",
    "plt.plot(Reward, label='reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will never use this poor taxi service again! --> I will never use this poor taxi service again!</s> Reward: 0.9977745413780212 NEGATIVE --> POSITIVE\n",
      "['I will never use this poor taxi service again!</s>']\n"
     ]
    }
   ],
   "source": [
    "#lets try that again but we only care creating sentences with opposite sentiment\n",
    "\n",
    "import pfrl\n",
    "from textrl import TextRLEnv, TextRLActor, train_agent_with_evaluation\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import logging\n",
    "import sys\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "#sentiment = pipeline('sentiment-analysis',model=\"cardiffnlp/twitter-roberta-base-sentiment\",tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment\",device=0,return_all_scores=True)\n",
    "sentiment = pipeline('sentiment-analysis', model=\"siebert/sentiment-roberta-large-english\", tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "scores_data = []\n",
    "\n",
    "class MyRLEnv(TextRLEnv):\n",
    "    def get_reward(self, input_item, predicted_list, finish): # predicted will be the list of predicted token\n",
    "      reward = 0\n",
    "      if finish:\n",
    "         for i in range(0,len(predicted_list)):\n",
    "            predicted_text = tokenizer.convert_tokens_to_string(predicted_list[i])\n",
    "\n",
    "            #tokens = tokenizer.tokenize(predicted_text)\n",
    "            pred_sen = sentiment(predicted_text)\n",
    "            sen = pred_sen[0]['label']\n",
    "            input_sen = input_item['label']\n",
    "               \n",
    "            sentiment_socre = pred_sen[0]['score']\n",
    "            if sen == input_sen: \n",
    "               sentiment_socre = -sentiment_socre\n",
    "               \n",
    "            reward += sentiment_socre\n",
    "         \n",
    "         print(input_item['input'],\"-->\",predicted_text, \"Reward:\", reward, input_sen, \"-->\", sen)\n",
    "         scores_data.append({'Sentiment':sentiment_socre, 'Reward':reward})\n",
    "         \n",
    "      return reward\n",
    "    def eval(self, input_item, predicted_list, finish):\n",
    "        predicted_text = tokenizer.convert_tokens_to_string(predicted_list[0])\n",
    "        pred_sen = sentiment(predicted_text)\n",
    "        sen = pred_sen[0]['label']\n",
    "        input_sen = input_item['label']\n",
    "        if sen == input_sen:\n",
    "           return True\n",
    "        else:\n",
    "           return False\n",
    "\n",
    "observaton_list = train_data\n",
    "\n",
    "env = MyRLEnv(model, tokenizer, observation_input=observaton_list, compare_sample=1)\n",
    "actor = TextRLActor(env,model,tokenizer,optimizer='adamw',\n",
    "                    temperature=0.8,\n",
    "                    top_k=0,\n",
    "                    top_p=0.85,\n",
    "                    repetition_penalty=2.0\n",
    "                    )\n",
    "agent = actor.agent_ppo(update_interval=50, minibatch_size=3, epochs=10,lr=3e-4)\n",
    "\n",
    "print(actor.predict(observaton_list[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ate it but will definitely not go there again! --> I will never go back to the buffet.</s> Reward: -0.9986951947212219 NEGATIVE --> NEGATIVE\n",
      "have not had a problem with the car since. --> I have been driving it for about a year now.</s> Reward: 0.6487700939178467 POSITIVE --> NEGATIVE\n",
      "some times they did not have things we ordered from the menu. --> The food is great.</s> Reward: 0.996487021446228 NEGATIVE --> POSITIVE\n",
      "the office staff were so nice and efficient. --> The staff was very friendly and helpful.</s> Reward: -0.9985264539718628 POSITIVE --> POSITIVE\n",
      "it was _num_ strikes and you're out. --> vs. joe</s> Reward: 0.9983289837837219 NEGATIVE --> POSITIVE\n",
      "amazing service. --> Great service.</s> Reward: -0.990301787853241 POSITIVE --> POSITIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pfrl/agents/ppo.py:133: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  actions = torch.tensor([b[\"action\"] for b in dataset], device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing special. --> I was in the car and I was unable to get out. The driver, who was very polite, was very nice and helpful. I would recommend this place to anyone looking for a great place to stay.</s> Reward: -0.9676669239997864 NEGATIVE --> NEGATIVE\n",
      "after drying it off, another guy got in to park it. --> Then he got in to park it.</s> Reward: 0.9927362203598022 NEGATIVE --> POSITIVE\n",
      "the product and quality is awesome! --> Great service and excellent product!</s> Reward: -0.9960330128669739 POSITIVE --> POSITIVE\n",
      "they are honest, reliable, forthright, and they do great work. --> they are honest, reliable, forthright, and they do great work.</s> Reward: 0.9961698651313782 POSITIVE --> NEGATIVE\n",
      "not coming back here -i do n't need the attitude. --> I hate being a little cold.</s> Reward: 0.9837327599525452 NEGATIVE --> POSITIVE\n",
      "big airport, lots of stores, eats and drinks. --> a big airport, lots of stores, eats and drinks.</s> Reward: -0.9949735403060913 POSITIVE --> POSITIVE\n",
      "the thai pb & j are amazing too. --> The thai pb & j is awesome too.</s> Reward: -0.9906620383262634 POSITIVE --> POSITIVE\n",
      "this was the only time i've requested to send the food back. --> I'm not sure what happened to the food.</s> Reward: -0.9965624213218689 NEGATIVE --> NEGATIVE\n",
      "we were served quickly and both enjoyed out delicious omelet's. --> The breakfast was very good. We were seated right away and both of us enjoyed the food. The service was fast and efficient.</s> Reward: 0.9982234835624695 POSITIVE --> NEGATIVE\n",
      "my order was scheduled for delivery on may _num_, but was not delivered. --> _num_</s> Reward: 0.9866662621498108 NEGATIVE --> POSITIVE\n",
      "will definitely be going back soon! --> The food is delicious!</s> Reward: -0.9960780739784241 POSITIVE --> POSITIVE\n",
      "when we asked the bus boy for more water he refused to get it. --> He was very rude.</s> Reward: 0.9942438006401062 NEGATIVE --> POSITIVE\n",
      "the service is as great as the cuisine. --> Great place to stay.</s> Reward: 0.9607569575309753 POSITIVE --> NEGATIVE\n",
      "love it. --> I love this movie.</s> Reward: -0.9954094290733337 POSITIVE --> POSITIVE\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (3, 1, 32128)) of distribution Categorical(logits: torch.Size([3, 1, 32128])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[[nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n       grad_fn=<SubBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#train agent\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pfrl\u001b[39m.\u001b[39;49mexperiments\u001b[39m.\u001b[39;49mtrain_agent_with_evaluation(\n\u001b[1;32m      4\u001b[0m     agent,\n\u001b[1;32m      5\u001b[0m     env,\n\u001b[1;32m      6\u001b[0m     steps\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     eval_n_steps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m     eval_n_episodes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     train_max_episode_len\u001b[39m=\u001b[39;49m\u001b[39m40000\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     eval_interval\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     outdir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mDualRLStyleTransfer/checkpoint\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/experiments/train_agent.py:208\u001b[0m, in \u001b[0;36mtrain_agent_with_evaluation\u001b[0;34m(agent, env, steps, eval_n_steps, eval_n_episodes, eval_interval, outdir, checkpoint_freq, train_max_episode_len, step_offset, eval_max_episode_len, eval_env, successful_score, step_hooks, evaluation_hooks, save_best_so_far_agent, use_tensorboard, eval_during_episode, logger)\u001b[0m\n\u001b[1;32m    191\u001b[0m     eval_max_episode_len \u001b[39m=\u001b[39m train_max_episode_len\n\u001b[1;32m    193\u001b[0m evaluator \u001b[39m=\u001b[39m Evaluator(\n\u001b[1;32m    194\u001b[0m     agent\u001b[39m=\u001b[39magent,\n\u001b[1;32m    195\u001b[0m     n_steps\u001b[39m=\u001b[39meval_n_steps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     logger\u001b[39m=\u001b[39mlogger,\n\u001b[1;32m    206\u001b[0m )\n\u001b[0;32m--> 208\u001b[0m eval_stats_history \u001b[39m=\u001b[39m train_agent(\n\u001b[1;32m    209\u001b[0m     agent,\n\u001b[1;32m    210\u001b[0m     env,\n\u001b[1;32m    211\u001b[0m     steps,\n\u001b[1;32m    212\u001b[0m     outdir,\n\u001b[1;32m    213\u001b[0m     checkpoint_freq\u001b[39m=\u001b[39;49mcheckpoint_freq,\n\u001b[1;32m    214\u001b[0m     max_episode_len\u001b[39m=\u001b[39;49mtrain_max_episode_len,\n\u001b[1;32m    215\u001b[0m     step_offset\u001b[39m=\u001b[39;49mstep_offset,\n\u001b[1;32m    216\u001b[0m     evaluator\u001b[39m=\u001b[39;49mevaluator,\n\u001b[1;32m    217\u001b[0m     successful_score\u001b[39m=\u001b[39;49msuccessful_score,\n\u001b[1;32m    218\u001b[0m     step_hooks\u001b[39m=\u001b[39;49mstep_hooks,\n\u001b[1;32m    219\u001b[0m     eval_during_episode\u001b[39m=\u001b[39;49meval_during_episode,\n\u001b[1;32m    220\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m agent, eval_stats_history\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/experiments/train_agent.py:64\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, steps, outdir, checkpoint_freq, max_episode_len, step_offset, evaluator, successful_score, step_hooks, eval_during_episode, logger)\u001b[0m\n\u001b[1;32m     62\u001b[0m episode_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     63\u001b[0m reset \u001b[39m=\u001b[39m episode_len \u001b[39m==\u001b[39m max_episode_len \u001b[39mor\u001b[39;00m info\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mneeds_reset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 64\u001b[0m agent\u001b[39m.\u001b[39;49mobserve(obs, r, done, reset)\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m step_hooks:\n\u001b[1;32m     67\u001b[0m     hook(env, agent, t)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/agent.py:164\u001b[0m, in \u001b[0;36mBatchAgent.observe\u001b[0;34m(self, obs, reward, done, reset)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobserve\u001b[39m(\u001b[39mself\u001b[39m, obs: Any, reward: \u001b[39mfloat\u001b[39m, done: \u001b[39mbool\u001b[39m, reset: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_observe([obs], [reward], [done], [reset])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/agents/ppo.py:684\u001b[0m, in \u001b[0;36mPPO.batch_observe\u001b[0;34m(self, batch_obs, batch_reward, batch_done, batch_reset)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_observe\u001b[39m(\u001b[39mself\u001b[39m, batch_obs, batch_reward, batch_done, batch_reset):\n\u001b[1;32m    683\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 684\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_observe_train(batch_obs, batch_reward, batch_done, batch_reset)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_observe_eval(batch_obs, batch_reward, batch_done, batch_reset)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/agents/ppo.py:810\u001b[0m, in \u001b[0;36mPPO._batch_observe_train\u001b[0;34m(self, batch_obs, batch_reward, batch_done, batch_reset)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m indices_that_ended:\n\u001b[1;32m    806\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_recurrent_states \u001b[39m=\u001b[39m mask_recurrent_state_at(\n\u001b[1;32m    807\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_recurrent_states, indices_that_ended\n\u001b[1;32m    808\u001b[0m         )\n\u001b[0;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_if_dataset_is_ready()\n",
      "File \u001b[0;32m/textRL/textrl/actor.py:249\u001b[0m, in \u001b[0;36mTextPPO._update_if_dataset_is_ready\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m     dataset \u001b[39m=\u001b[39m pfrl\u001b[39m.\u001b[39magents\u001b[39m.\u001b[39mppo\u001b[39m.\u001b[39m_make_dataset(\n\u001b[1;32m    239\u001b[0m         episodes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory,\n\u001b[1;32m    240\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(dataset) \u001b[39m==\u001b[39m dataset_size\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(dataset)\n\u001b[1;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplained_variance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_explained_variance(\n\u001b[1;32m    251\u001b[0m     \u001b[39mlist\u001b[39m(itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory))\n\u001b[1;32m    252\u001b[0m )\n\u001b[1;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/agents/ppo.py:490\u001b[0m, in \u001b[0;36mPPO._update\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    488\u001b[0m     states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_normalizer(states, update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    489\u001b[0m actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([b[\u001b[39m\"\u001b[39m\u001b[39maction\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m--> 490\u001b[0m distribs, vs_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(states)\n\u001b[1;32m    492\u001b[0m advs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    493\u001b[0m     [b[\u001b[39m\"\u001b[39m\u001b[39madv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    495\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstandardize_advantages:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/nn/branched.py:30\u001b[0m, in \u001b[0;36mBranched.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward the arguments to the child modules.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m        tuple: Tuple of the returned values from the child modules.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(mod(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;49;00m mod \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchild_modules)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pfrl/nn/branched.py:30\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward the arguments to the child modules.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m        tuple: Tuple of the returned values from the child modules.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(mod(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild_modules)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/textRL/textrl/actor.py:208\u001b[0m, in \u001b[0;36mSoftmaxCategoricalHead.forward\u001b[0;34m(self, logits)\u001b[0m\n\u001b[1;32m    206\u001b[0m logits \u001b[39m=\u001b[39m logits \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature\n\u001b[1;32m    207\u001b[0m logits \u001b[39m=\u001b[39m top_k_top_p_filtering(logits, top_k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_k, top_p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_p)\n\u001b[0;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mCategorical(logits\u001b[39m=\u001b[39;49mlogits)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributions/distribution.py:62\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     61\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 62\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (3, 1, 32128)) of distribution Categorical(logits: torch.Size([3, 1, 32128])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[[nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n       grad_fn=<SubBackward0>)"
     ]
    }
   ],
   "source": [
    "#train agent\n",
    "\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=5000,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,\n",
    "    train_max_episode_len=40000,\n",
    "    eval_interval=1,\n",
    "    outdir='DualRLStyleTransfer/checkpoint',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save hugingface models\n",
    "model.save_pretrained(\"DualRLStyleTransfer/checkpoint/model_flan-t5-base\")\n",
    "tokenizer.save_pretrained(\"DualRLStyleTransfer/checkpoint/tokenizer_flan-t5-base\")\n",
    "sentiment.save_pretrained(\"DualRLStyleTransfer/checkpoint/sentiment_roberta-large-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hugingface models\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import logging\n",
    "import sys\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DualRLStyleTransfer/checkpoint/tokenizer_flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"DualRLStyleTransfer/checkpoint/model_flan-t5-base\")\n",
    "model.eval()\n",
    "model.cuda()\n",
    "sentiment = pipeline('sentiment-analysis', model=\"DualRLStyleTransfer/checkpoint/sentiment_roberta-large-english\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m tokenizer(context, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, num_beams\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      6\u001b[0m output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1322\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1315\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1317\u001b[0m         )\n\u001b[1;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1320\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1323\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1324\u001b[0m     )\n\u001b[1;32m   1326\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:638\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    636\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    637\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 638\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    640\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py:985\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    984\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 985\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_tokens(input_ids)\n\u001b[1;32m    987\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m    989\u001b[0m \u001b[39m# required mask seq length can be calculated via length of past\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "context = \"I love this movie so much! It is so good!\"\n",
    "input = tokenizer(context, return_tensors=\"pt\")\n",
    "input = input.to('cuda')\n",
    "output = model.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_length=100, num_beams=5, early_stopping=True)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)\n",
    "\n",
    "input_ids = input.input_ids\n",
    "outputs = model.generate(input_ids=input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "#test sentiment\n",
    "print(sentiment.predict(context))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe diesen Film!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"Das Haus ist wunderbar.\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()\n",
    "\n",
    "context = \"I love this movie so much! It is so good!\"\n",
    "input = tokenizer(context, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(input_ids=input, max_length=100, num_beams=5, early_stopping=True)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
